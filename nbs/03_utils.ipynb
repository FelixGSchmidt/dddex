{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fcaa-9d2d-49ea-8cc3-b7e7ebd4b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d834faa-e24c-4542-9070-b8ec8562cc63",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101f7c6-4c66-481f-b580-a8dda687c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294fa26-4c16-435e-9d6a-b7537d8c4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24f328-6b14-4ae8-8034-c24c4a128b28",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e0e08-4fd9-4644-8048-d9f0948600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3cf6b-c0ef-4763-a49c-e1c26479622e",
   "metadata": {},
   "source": [
    "## Restructure Weights Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828150c-4075-4501-8e91-f107c9c90aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def restructureWeightsDataList(weightsDataList, outputType = 'onlyPositiveWeights', y = None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, yUnique = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                              yWeightPos = yWeightPos,\n",
    "                                                              equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yUnique = yUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, yUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistribution':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            indicesSort = np.argsort(yWeightPos)\n",
    "            \n",
    "            weightsPosSorted = weightsPos[indicesSort]\n",
    "            yWeightPosSorted = yWeightPos[indicesSort]\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsPosSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yWeightPosSorted = yWeightPosSorted * scalingList[i]\n",
    "            \n",
    "            distributionDataList.append((cumulativeProbs, yWeightPosSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistributionSummarized':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarizedSorted, yPosWeightUniqueSorted = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                                                   yWeightPos = yWeightPos,\n",
    "                                                                                   equalWeights = equalWeights)\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsSummarizedSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yPosWeightUniqueSorted = yPosWeightUniqueSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, yPosWeightUniqueSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02cf86-4954-4f80-bff1-23a9980a6688",
   "metadata": {},
   "source": [
    "### Summarize Weights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd1e41-454a-4921-a5e8-49c6278afd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarizeWeightsData(weightsPos, yWeightPos, equalWeights = False):\n",
    "    \n",
    "    if equalWeights:\n",
    "        counterDict = Counter(yWeightPos)\n",
    "        yUniqueSorted = np.sort(list(counterDict.keys()))\n",
    "\n",
    "        weightsSummarizedSorted = np.array([counterDict[value] / len(yWeightPos) for value in yUniqueSorted])\n",
    "    \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "\n",
    "        for i, yValue in enumerate(yWeightPos):\n",
    "            duplicationDict[yValue].append(i)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        yUnique = list()\n",
    "\n",
    "        for value, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            yUnique.append(value)\n",
    "\n",
    "        weightsSummarized, yUnique = np.array(weightsSummarized), np.array(yUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        indicesSort = np.argsort(yUnique)\n",
    "        weightsSummarizedSorted, yUniqueSorted = weightsSummarized[indicesSort], yUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, yUniqueSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f316b2-dd73-4a57-aad7-ec6145bae9df",
   "metadata": {},
   "source": [
    "## Grouped Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5b30e-671f-4ce4-910c-f8eac47aee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function creates the cross-validation folds for every time series. Usually you'd want all test-observations \n",
    "# of each fold to refer to the same time period, but this is impossible to ensure in the case of the two-step models,\n",
    "# because the regression of the non-zero observations will always contain data of different time points. For that\n",
    "# reason, we refrain from trying to ensure this consistency.\n",
    "# Instead we organize our splits such that we always move a fixed amount of observations into the future from split\n",
    "# to split for every time series. This fixed amount of observations is currently set to the test length of the\n",
    "# corresponding time series.\n",
    "\n",
    "# In case this function is supposed to be used in the two-step case, data simply has to be filtered before hand\n",
    "# to only contain positive demand observations.\n",
    "\n",
    "def groupedTimeSeriesSplit(data, kFolds, testLength, groupFeature, timeFeature):\n",
    "    \n",
    "    # We reset the index because we have to access 'group.index' later on and\n",
    "    # want to make sure that we return the implicit numerical indices for our splits.\n",
    "    data = data.reset_index(drop = True)\n",
    "\n",
    "    dataGrouped = data.groupby(groupFeature)\n",
    "    splitNumbers = np.flip(np.array(range(kFolds)))\n",
    "    \n",
    "    foldsList = list()\n",
    "\n",
    "    for i in splitNumbers:\n",
    "\n",
    "        trainIndicesList = list()\n",
    "        valIndicesList = list()\n",
    "\n",
    "        valIndicesDict = dict()\n",
    "\n",
    "        for name, group in dataGrouped:\n",
    "\n",
    "            timeMin = int(group[timeFeature].min())\n",
    "            timeMax = int(group[timeFeature].max())\n",
    "\n",
    "            validationTimeMax = timeMax - i * testLength\n",
    "            trainTimeMax = validationTimeMax - testLength\n",
    "\n",
    "            trainTimesGroup = np.array(range(timeMin, trainTimeMax + 1))\n",
    "            valTimesGroup = np.array(range(trainTimeMax + 1, validationTimeMax + 1))\n",
    "\n",
    "            trainIndicesCheck = [timePoint in trainTimesGroup for timePoint in group[timeFeature]]\n",
    "            valIndicesCheck = [timePoint in valTimesGroup for timePoint in group[timeFeature]]\n",
    "\n",
    "            trainIndicesGroup = group.index[trainIndicesCheck]\n",
    "            valIndicesGroup = group.index[valIndicesCheck]\n",
    "\n",
    "            trainIndicesList.append(trainIndicesGroup)\n",
    "            valIndicesList.append(valIndicesGroup)\n",
    "\n",
    "        trainIndices = np.concatenate(trainIndicesList)\n",
    "        valIndices = np.concatenate(valIndicesList)\n",
    "        fold = (trainIndices, valIndices)\n",
    "\n",
    "        foldsList.append(fold)\n",
    "\n",
    "    return foldsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d688792-2db3-476c-a73a-f1088f4880ee",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940af8bb-aa80-4f46-adef-c3fd6a424a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generateFinalOutput(dataOriginal, \n",
    "                        dataDecisions, \n",
    "                        targetVariable = 'demand', \n",
    "                        mergeOn = None, \n",
    "                        variablesToAdd = None, \n",
    "                        scaleBy = None, \n",
    "                        includeTraining = False, \n",
    "                        sortBy = None,\n",
    "                        longFormat = False,\n",
    "                        **kwargs):\n",
    "    \n",
    "    dataOriginal = dataOriginal.rename(columns = {targetVariable: 'actuals'})\n",
    "    \n",
    "    if not scaleBy is None:\n",
    "        if not isinstance(scaleBy, str): \n",
    "            raise ValueError(\"'scaleBy' has to a string specifying a single feature to scale the target variable!\")\n",
    "        elif scaleBy in dataOriginal.columns:\n",
    "            dataOriginal['actuals'] = dataOriginal['actuals'] * dataOriginal[scaleBy]\n",
    "        else:\n",
    "            raise ValueError(f\"The specified feature {scaleBy} is not part of 'dataOriginal'!\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Adding additional data to the matrix dataDecisions\n",
    "    # NOTE: This function is written to be useable for all datasets we currently use. \n",
    "    colsToAdd = ['id', 'sku_code_prefix', 'sku_code', 'SKU_API',\n",
    "                 'actuals', 'revenue', 'label',\n",
    "                 'adi', 'adi_sku', 'adi_product', 'cv2', 'cvDemand_sku', 'cvDemand_product']\n",
    "    \n",
    "    if isinstance(mergeOn, list):\n",
    "        colsToAdd = colsToAdd + mergeOn\n",
    "    else:\n",
    "        colsToAdd.append(mergeOn)\n",
    "    \n",
    "    if isinstance(variablesToAdd, list):\n",
    "        colsToAdd = colsToAdd + variablesToAdd\n",
    "    else:\n",
    "        colsToAdd.append(variablesToAdd)\n",
    "        \n",
    "    if isinstance(sortBy, list):\n",
    "        colsToAdd = colsToAdd + sortBy\n",
    "    else:\n",
    "        colsToAdd.append(sortBy)\n",
    "    \n",
    "    colsToAdd = pd.Series(colsToAdd)\n",
    "    colsToAdd = colsToAdd[colsToAdd.isin(dataOriginal.columns)] \n",
    "    colsToAdd = colsToAdd.unique()\n",
    "    \n",
    "    dataTestInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'test', colsToAdd].reset_index(drop = True)\n",
    "    \n",
    "    if not longFormat:\n",
    "\n",
    "        if mergeOn is None:\n",
    "            dataResults = pd.concat([dataTestInfoToAdd, dataDecisions.reset_index(drop = True)], axis = 1)\n",
    "        else:\n",
    "            dataResults = pd.merge(dataTestInfoToAdd, dataDecisions, on = mergeOn)\n",
    "\n",
    "        #---\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dataDecisionsStacked = dataDecisions.stack().reset_index().set_index('level_0')\n",
    "        dataDecisionsStacked.rename(columns = {'level_1': 'decisionType'}, inplace = True)\n",
    "        \n",
    "        dataDecisionsStacked.rename(columns = {0: 'decisions'}, inplace = True)\n",
    "        dataDecisionsStacked.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        numberOfDecisionTypes = len(dataDecisionsStacked['decisionType'].unique())\n",
    "        \n",
    "        infoDuplicatedDf = dataTestInfoToAdd.loc[dataTestInfoToAdd.index.repeat(numberOfDecisionTypes)]\n",
    "        infoDuplicatedDf.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        dataResults = pd.concat([infoDuplicatedDf, dataDecisionsStacked], axis = 1)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    infoToAdd = pd.DataFrame(kwargs, index = [0])\n",
    "    infoToAdd['label'] = 'test'\n",
    "    dataResults = pd.merge(dataResults, infoToAdd, on = 'label')\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if includeTraining:\n",
    "        dataTrainInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'train', colsToAdd].reset_index(drop = True)\n",
    "        dataResults = pd.concat([dataTrainInfoToAdd, dataResults], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if not sortBy is None:\n",
    "        \n",
    "        if not all([sortByCol in dataResults.columns for sortByCol in sortBy]):\n",
    "            raise ValueError(\"Columns specified by 'sortBy' must be part of 'dataOriginal'!\")\n",
    "        else:\n",
    "            dataResults.sort_values(by = sortBy, axis = 0, inplace = True, ignore_index = True)\n",
    "    \n",
    "    return dataResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
