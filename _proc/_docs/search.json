[
  {
    "objectID": "baseweightspredictor.html#restructure-weights-data-list",
    "href": "baseweightspredictor.html#restructure-weights-data-list",
    "title": "core",
    "section": "Restructure Weights Data List",
    "text": "Restructure Weights Data List\n\nsource\n\nrestructureWeightsDataList\n\n restructureWeightsDataList (weightsDataList,\n                             outputType='onlyPositiveWeights', Y=None,\n                             scalingList=None, equalWeights=False)\n\nHelper function. Creates weights-output by specifying considered neighbors of training observations for every test observation of interest.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nweightsDataList\n\n\n\n\n\noutputType\nstr\nonlyPositiveWeights\nSpecifies the structure of the output. - If “all”, then the weights are outputted as an array that is exactly as long as  the number of training observations. Consequently, also weights equal to zero are being computed.  NOTE: This can be take up lots of RAM for large datasets with > 10^6 observations.- If “onlyPositiveWeights”, then weights equal to zero are truncated. In order to be  able to identify to which training observation each weight belongs, a tuple is outputted whose first entry are the weights and the second one are the corresponding training indices. - If “summarized”, then additionally to “onlyPositiveWeights”, weights referencing to the same y-value are condensed to one single weight. In this case, the second entry of the outputted tuple contains the y-values to which each weight corresponds.  NOTE: Summarizing the weights can be very computationally burdensome if roughly the considered dataset has more than 10^6 observations and if binSize > 10^4.- If “cumulativeDistributionSummarized”, then additionally to “summarized”, the cumulative sum of the weights is computed, which can be interpreted as the empirical cumulative distribution function given the feature vector at hand. NOTE: This output type requires summarizing the weights, which can be very computationally  burdensome if roughly the considered dataset has more than 10^6 observations and if  binSize > 10^4.\n\n\nY\nNoneType\nNone\n\n\n\nscalingList\nNoneType\nNone\n\n\n\nequalWeights\nbool\nFalse\n\n\n\n\n\n\nSummarize Weights Data\n\nsource\n\n\nsummarizeWeightsData\n\n summarizeWeightsData (weightsPos, YWeightPos, equalWeights=False)"
  },
  {
    "objectID": "wsaa.html#random-forest",
    "href": "wsaa.html#random-forest",
    "title": "wSAA",
    "section": "Random Forest",
    "text": "Random Forest\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n\nsource\n\nRandomForestWSAA\n\n RandomForestWSAA (n_estimators=100, criterion='mse', max_depth=None,\n                   min_samples_split=2, min_samples_leaf=1,\n                   min_weight_fraction_leaf=0.0, max_features='auto',\n                   max_leaf_nodes=None, min_impurity_decrease=0.0,\n                   min_impurity_split=None, bootstrap=True,\n                   oob_score=False, n_jobs=None, random_state=None,\n                   verbose=0, warm_start=False, ccp_alpha=0.0,\n                   max_samples=None)\n\nA random forest regressor.\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\nRead more in the :ref:User Guide <forest>.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_estimators\nint\n100\nThe number of trees in the forest... versionchanged:: 0.22 The default value of n_estimators changed from 10 to 100 in 0.22.\n\n\ncriterion\nstr\nmse\nThe function to measure the quality of a split. Supported criteriaare “mse” for the mean squared error, which is equal to variancereduction as feature selection criterion, and “mae” for the meanabsolute error... versionadded:: 0.18 Mean Absolute Error (MAE) criterion.\n\n\nmax_depth\nNoneType\nNone\nThe maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.\n\n\nmin_samples_split\nint\n2\nThe minimum number of samples required to split an internal node:- If int, then consider min_samples_split as the minimum number.- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split... versionchanged:: 0.18 Added float values for fractions.\n\n\nmin_samples_leaf\nint\n1\nThe minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast min_samples_leaf training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider min_samples_leaf as the minimum number.- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node... versionchanged:: 0.18 Added float values for fractions.\n\n\nmin_weight_fraction_leaf\nfloat\n0.0\nThe minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.\n\n\nmax_features\nstr\nauto\nThe number of features to consider when looking for the best split:- If int, then consider max_features features at each split.- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.- If “auto”, then max_features=n_features.- If “sqrt”, then max_features=sqrt(n_features).- If “log2”, then max_features=log2(n_features).- If None, then max_features=n_features.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than max_features features.\n\n\nmax_leaf_nodes\nNoneType\nNone\nGrow trees with max_leaf_nodes in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.\n\n\nmin_impurity_decrease\nfloat\n0.0\nA node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)where N is the total number of samples, N_t is the number ofsamples at the current node, N_t_L is the number of samples in theleft child, and N_t_R is the number of samples in the right child.N, N_t, N_t_R and N_t_L all refer to the weighted sum,if sample_weight is passed... versionadded:: 0.19\n\n\nmin_impurity_split\nNoneType\nNone\nThreshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19 min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split has changed from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.\n\n\nbootstrap\nbool\nTrue\nWhether bootstrap samples are used when building trees. If False, thewhole dataset is used to build each tree.\n\n\noob_score\nbool\nFalse\nwhether to use out-of-bag samples to estimatethe R^2 on unseen data.\n\n\nn_jobs\nNoneType\nNone\nThe number of jobs to run in parallel. :meth:fit, :meth:predict,:meth:decision_path and :meth:apply are all parallelized over thetrees. None means 1 unless in a :obj:joblib.parallel_backendcontext. -1 means using all processors. See :term:Glossary<br><n_jobs> for more details.\n\n\nrandom_state\nNoneType\nNone\nControls both the randomness of the bootstrapping of the samples usedwhen building trees (if bootstrap=True) and the sampling of thefeatures to consider when looking for the best split at each node(if max_features < n_features).See :term:Glossary <random_state> for details.\n\n\nverbose\nint\n0\nControls the verbosity when fitting and predicting.\n\n\nwarm_start\nbool\nFalse\nWhen set to True, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:the Glossary <warm_start>.\n\n\nccp_alpha\nfloat\n0.0\nComplexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller thanccp_alpha will be chosen. By default, no pruning is performed. See:ref:minimal_cost_complexity_pruning for details... versionadded:: 0.22\n\n\nmax_samples\nNoneType\nNone\nIf bootstrap is True, the number of samples to draw from Xto train each base estimator.- If None (default), then draw X.shape[0] samples.- If int, then draw max_samples samples.- If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0, 1)... versionadded:: 0.22"
  },
  {
    "objectID": "levelsetforecaster.html#generate-bins",
    "href": "levelsetforecaster.html#generate-bins",
    "title": "LSF",
    "section": "Generate Bins",
    "text": "Generate Bins\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nOutput \n---------- in \nUsed to generate the bin-structure induced by the Level-Set-Forecaster algorithm for\n``neighborStrategy == 'bins'``....\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nNotes \n---------- in \nUsed to generate the bin-structure induced by the Level-Set-Forecaster algorithm for\n``neighborStrategy == 'bins'``....\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Output\n  else: warn(msg)\n/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\ngenerateBins\n\n generateBins (binSize, YPredTrain)\n\nUsed to generate the bin-structure induced by the Level-Set-Forecaster algorithm for neighborStrategy == 'bins'. Bins are created by starting at the lowest value of YPredTrain and then succesively adding the closest next prediction to the current bin until binSize-many observations have been allocated. Then the generation of a new bin is started in the same manner until all values of YPredTrain have been assigned to exactly one bin.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nbinSize\nint\nThe size of each bin that is being created. Every bin is going to have this sizeapart from the one containing the predictions with the highest values (see remarks)\n\n\nYPredTrain\narray\nThe predictions for the training observations."
  },
  {
    "objectID": "levelsetforecaster.html#scores-for-single-fold",
    "href": "levelsetforecaster.html#scores-for-single-fold",
    "title": "LSF",
    "section": "Scores for Single Fold",
    "text": "Scores for Single Fold\n\nsource\n\nscoresForFold\n\n scoresForFold (binSizeGrid, probs, foldNumber, cvFolds, estimator,\n                LSF_type, Y, X)\n\n\n\nCoefficient of Prescriptiveness\n\nsource\n\n\ngetCoefPres\n\n getCoefPres (decisions, decisionsSAA, YTest, prob)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DDDEx",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "DDDEx",
    "section": "Install",
    "text": "Install\npip install lsf"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "DDDEx",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  }
]