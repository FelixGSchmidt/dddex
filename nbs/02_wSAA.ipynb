{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9df2e7-fdc8-44e7-b16c-d8b15ed01db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbaad6-3667-46e6-b25e-efa0822452ae",
   "metadata": {},
   "source": [
    "# wSAA\n",
    "\n",
    "> Module description for wSAA classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c687b2c-941e-4e2a-9855-aa227ccb8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp wSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1faac0-5c0d-4c70-80e9-7d959811b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e786a-e401-43d8-91e9-97c175eba23a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f92e6-67ad-492b-9779-7b9acfc3d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from dddex.core import BaseWeightsBasedPredictor, restructureWeightsDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bd1d-ceaa-4b36-a42e-aaa2e4062c22",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b775c4-ca57-4539-a0b8-e282bdf963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "class RandomForestWSAA(RandomForestRegressor, BaseWeightsBasedPredictor):\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        super(RandomForestRegressor, self).fit(X = X, y = Y)\n",
    "        \n",
    "        self.Y = Y\n",
    "        self.leafIndicesTrain = self.apply(X)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self, X):\n",
    "        \n",
    "        preds = super(RandomForestRegressor, self).predict(X)\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db18d0f-bdb7-4f08-a8cf-ac07a49e7d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n",
      "  else: warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='mse', max_depth=None,\n",
       ">                        min_samples_split=2, min_samples_leaf=1,\n",
       ">                        min_weight_fraction_leaf=0.0, max_features='auto',\n",
       ">                        max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       ">                        min_impurity_split=None, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | mse | The function to measure the quality of a split. Supported criteria<br>are \"mse\" for the mean squared error, which is equal to variance<br>reduction as feature selection criterion, and \"mae\" for the mean<br>absolute error.<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | str | auto | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `int(max_features * n_features)` features are considered at each<br>  split.<br>- If \"auto\", then `max_features=n_features`.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None, then `max_features=n_features`.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| min_impurity_split | NoneType | None | Threshold for early stopping in tree growth. A node will split<br>if its impurity is above the threshold, otherwise it is a leaf.<br><br>.. deprecated:: 0.19<br>   ``min_impurity_split`` has been deprecated in favor of<br>   ``min_impurity_decrease`` in 0.19. The default value of<br>   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it<br>   will be removed in 0.25. Use ``min_impurity_decrease`` instead. |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | whether to use out-of-bag samples to estimate<br>the R\\^2 on unseen data. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`the Glossary <warm_start>`. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max_samples * X.shape[0]` samples. Thus,<br>  `max_samples` should be in the interval `(0, 1)`.<br><br>.. versionadded:: 0.22 |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='mse', max_depth=None,\n",
       ">                        min_samples_split=2, min_samples_leaf=1,\n",
       ">                        min_weight_fraction_leaf=0.0, max_features='auto',\n",
       ">                        max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       ">                        min_impurity_split=None, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | mse | The function to measure the quality of a split. Supported criteria<br>are \"mse\" for the mean squared error, which is equal to variance<br>reduction as feature selection criterion, and \"mae\" for the mean<br>absolute error.<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | str | auto | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `int(max_features * n_features)` features are considered at each<br>  split.<br>- If \"auto\", then `max_features=n_features`.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None, then `max_features=n_features`.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| min_impurity_split | NoneType | None | Threshold for early stopping in tree growth. A node will split<br>if its impurity is above the threshold, otherwise it is a leaf.<br><br>.. deprecated:: 0.19<br>   ``min_impurity_split`` has been deprecated in favor of<br>   ``min_impurity_decrease`` in 0.19. The default value of<br>   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it<br>   will be removed in 0.25. Use ``min_impurity_decrease`` instead. |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | whether to use out-of-bag samples to estimate<br>the R\\^2 on unseen data. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`the Glossary <warm_start>`. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max_samples * X.shape[0]` samples. Thus,<br>  `max_samples` should be in the interval `(0, 1)`.<br><br>.. versionadded:: 0.22 |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RandomForestWSAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a240e3b-5acc-4f82-8f4d-29391f5692b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.fit\n",
       "\n",
       ">      RandomForestWSAA.fit (X, Y)\n",
       "\n",
       "Build a forest of trees from the training set (X, y).\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | {array-like, sparse matrix} of shape (n_samples, n_features) | The training input samples. Internally, its dtype will be converted<br>to ``dtype=np.float32``. If a sparse matrix is provided, it will be<br>converted into a sparse ``csc_matrix``. |\n",
       "| Y |  |  |\n",
       "| **Returns** | **object** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.fit\n",
       "\n",
       ">      RandomForestWSAA.fit (X, Y)\n",
       "\n",
       "Build a forest of trees from the training set (X, y).\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | {array-like, sparse matrix} of shape (n_samples, n_features) | The training input samples. Internally, its dtype will be converted<br>to ``dtype=np.float32``. If a sparse matrix is provided, it will be<br>converted into a sparse ``csc_matrix``. |\n",
       "| Y |  |  |\n",
       "| **Returns** | **object** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RandomForestWSAA.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915159d-b5ac-414f-9bd4-202c0e2da08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@patch\n",
    "def getWeightsData(self: RandomForestWSAA, \n",
    "                   X: np.ndarray, # Feature matrix for whose rows conditional density estimates are computed.\n",
    "                   outputType: 'all' | # Specifies structure of output.\n",
    "                               'onlyPositiveWeights' | \n",
    "                               'summarized' | \n",
    "                               'cumulativeDistribution' | \n",
    "                               'cumulativeDistributionSummarized' = 'onlyPositiveWeights', \n",
    "                   scalingList: list | np.ndarray | None = None, # List or array with same size as self.Y containing floats being multiplied with self.Y.\n",
    "                   ):\n",
    "\n",
    "    leafIndicesDf = self.apply(X)\n",
    "\n",
    "    weightsDataList = list()\n",
    "\n",
    "    for leafIndices in leafIndicesDf:\n",
    "        leafComparisonMatrix = (self.leafIndicesTrain == leafIndices) * 1\n",
    "        nObsInSameLeaf = np.sum(leafComparisonMatrix, axis = 0)\n",
    "\n",
    "        # It can happen that RF decides that the best strategy is to fit no tree at\n",
    "        # all and simply average all results (happens when min_child_sample is too high, for example).\n",
    "        # In this case 'leafComparisonMatrix' mustn't be averaged because there has been only a single tree.\n",
    "        if len(leafComparisonMatrix.shape) == 1:\n",
    "            weights = leafComparisonMatrix / nObsInSameLeaf\n",
    "        else:\n",
    "            weights = np.mean(leafComparisonMatrix / nObsInSameLeaf, axis = 1)\n",
    "\n",
    "        weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "        weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "    #---\n",
    "\n",
    "    weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                 outputType = outputType, \n",
    "                                                 Y = self.Y, \n",
    "                                                 scalingList = scalingList,\n",
    "                                                 equalWeights = False)\n",
    "\n",
    "    return weightsDataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe067-f51e-4038-ae00-06bafa7ca014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.getWeightsData\n",
       "\n",
       ">      RandomForestWSAA.getWeightsData (X:numpy.ndarray, outputType:Union[Forwar\n",
       ">                                       dRef('all'),ForwardRef('onlyPositiveWeig\n",
       ">                                       hts'),ForwardRef('summarized'),ForwardRe\n",
       ">                                       f('cumulativeDistribution'),ForwardRef('\n",
       ">                                       cumulativeDistributionSummarized')]='onl\n",
       ">                                       yPositiveWeights', scalingList:Union[lis\n",
       ">                                       t,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Compute weights of feature array X\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for whose rows conditional density estimates are computed. |\n",
       "| outputType | 'all' \\| 'onlyPositiveWeights' \\| 'summarized' \\| 'cumulativeDistribution' \\| 'cumulativeDistributionSummarized' | onlyPositiveWeights | Specifies structure of output. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.getWeightsData\n",
       "\n",
       ">      RandomForestWSAA.getWeightsData (X:numpy.ndarray, outputType:Union[Forwar\n",
       ">                                       dRef('all'),ForwardRef('onlyPositiveWeig\n",
       ">                                       hts'),ForwardRef('summarized'),ForwardRe\n",
       ">                                       f('cumulativeDistribution'),ForwardRef('\n",
       ">                                       cumulativeDistributionSummarized')]='onl\n",
       ">                                       yPositiveWeights', scalingList:Union[lis\n",
       ">                                       t,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Compute weights of feature array X\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for whose rows conditional density estimates are computed. |\n",
       "| outputType | 'all' \\| 'onlyPositiveWeights' \\| 'summarized' \\| 'cumulativeDistribution' \\| 'cumulativeDistributionSummarized' | onlyPositiveWeights | Specifies structure of output. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RandomForestWSAA.getWeightsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f259e0-c201-4074-85a8-468d3f3c3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def predict(self: RandomForestWSAA, \n",
    "            X: np.ndarray, # Feature matrix of samples for which an estimation of conditional quantiles is computed.\n",
    "            probs: list | np.ndarray = [0.1, 0.5, 0.9], # Probabilities for which the estimated conditional p-quantiles are computed.\n",
    "            outputAsDf: bool = False, # Output is either a dataframe with 'probs' as cols or a dict with 'probs' as keys.\n",
    "            scalingList: list | np.ndarray | None = None, # List or array with same size as self.Y containing floats being multiplied with self.Y.\n",
    "            ):\n",
    "\n",
    "    quantileRes = super(BaseWeightsBasedPredictor, self).predict(X = X,\n",
    "                                                                 probs = probs,\n",
    "                                                                 outputAsDf = outputAsDf,\n",
    "                                                                 scalingList = scalingList)\n",
    "\n",
    "    return quantileRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673580e-9772-44db-ab59-1f81ace73467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.predict\n",
       "\n",
       ">      RandomForestWSAA.predict (X:numpy.ndarray,\n",
       ">                                probs:Union[list,numpy.ndarray]=[0.1, 0.5,\n",
       ">                                0.9], outputAsDf:bool=False, scalingList:Union[\n",
       ">                                list,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Predict regression target for X.\n",
       "\n",
       "The predicted regression target of an input sample is computed as the\n",
       "mean predicted regression targets of the trees in the forest.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix of samples for which an estimation of conditional quantiles is computed. |\n",
       "| probs | list \\| np.ndarray | [0.1, 0.5, 0.9] | Probabilities for which the estimated conditional p-quantiles are computed. |\n",
       "| outputAsDf | bool | False | Output is either a dataframe with 'probs' as cols or a dict with 'probs' as keys. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |\n",
       "| **Returns** | **ndarray of shape (n_samples,) or (n_samples, n_outputs)** |  | **The predicted values.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA.predict\n",
       "\n",
       ">      RandomForestWSAA.predict (X:numpy.ndarray,\n",
       ">                                probs:Union[list,numpy.ndarray]=[0.1, 0.5,\n",
       ">                                0.9], outputAsDf:bool=False, scalingList:Union[\n",
       ">                                list,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Predict regression target for X.\n",
       "\n",
       "The predicted regression target of an input sample is computed as the\n",
       "mean predicted regression targets of the trees in the forest.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix of samples for which an estimation of conditional quantiles is computed. |\n",
       "| probs | list \\| np.ndarray | [0.1, 0.5, 0.9] | Probabilities for which the estimated conditional p-quantiles are computed. |\n",
       "| outputAsDf | bool | False | Output is either a dataframe with 'probs' as cols or a dict with 'probs' as keys. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |\n",
       "| **Returns** | **ndarray of shape (n_samples,) or (n_samples, n_outputs)** |  | **The predicted values.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RandomForestWSAA.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471bba6-50ec-49a9-980d-1c10f4361938",
   "metadata": {},
   "source": [
    "## SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b21e7-4c15-4c6d-8ebd-236740ec71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SAA(BaseWeightsBasedPredictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Y = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc67de5-0fcb-47b3-82dd-908cbedaae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA\n",
       "\n",
       ">      SAA ()\n",
       "\n",
       "Helper class that provides a standard way to create an ABC using\n",
       "inheritance."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA\n",
       "\n",
       ">      SAA ()\n",
       "\n",
       "Helper class that provides a standard way to create an ABC using\n",
       "inheritance."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd98f3-44c9-44f6-af9c-6430afa6fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: SAA, \n",
    "        Y: np.ndarray, # Target values which form the estimated density function based on the SAA algorithm.\n",
    "        ):\n",
    "    self.Y = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9520933-6480-4e96-bace-8b9f164cd922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA.fit\n",
       "\n",
       ">      SAA.fit (Y:numpy.ndarray)\n",
       "\n",
       "Fit weights-based predictor on given training data\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Y | np.ndarray | Target values which form the estimated density function based on the SAA algorithm. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA.fit\n",
       "\n",
       ">      SAA.fit (Y:numpy.ndarray)\n",
       "\n",
       "Fit weights-based predictor on given training data\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Y | np.ndarray | Target values which form the estimated density function based on the SAA algorithm. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SAA.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db11f98-11b2-420f-9420-0bcbd3ebf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def getWeightsData(self: SAA, \n",
    "                   X: np.ndarray, # Feature matrix for whose rows conditional density estimates are computed.\n",
    "                   outputType: 'all' | # Specifies structure of output.\n",
    "                               'onlyPositiveWeights' | \n",
    "                               'summarized' | \n",
    "                               'cumulativeDistribution' | \n",
    "                               'cumulativeDistributionSummarized' = 'onlyPositiveWeights', \n",
    "                   scalingList: list | np.ndarray | None = None, # List or array with same size as self.Y containing floats being multiplied with self.Y.\n",
    "                   ):\n",
    "\n",
    "    if X is None:\n",
    "        neighborsList = [np.arange(len(self.Y))]\n",
    "    else:\n",
    "        neighborsList = [np.arange(len(self.Y)) for i in range(X.shape[0])]\n",
    "\n",
    "    # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "    weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "    weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                 outputType = outputType, \n",
    "                                                 Y = self.Y,\n",
    "                                                 scalingList = scalingList,\n",
    "                                                 equalWeights = True)\n",
    "\n",
    "    return weightsDataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e77e0d-5cb8-4c29-b356-0f0e61bccde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA.getWeightsData\n",
       "\n",
       ">      SAA.getWeightsData (X:numpy.ndarray, outputType:Union[ForwardRef('all'),F\n",
       ">                          orwardRef('onlyPositiveWeights'),ForwardRef('summariz\n",
       ">                          ed'),ForwardRef('cumulativeDistribution'),ForwardRef(\n",
       ">                          'cumulativeDistributionSummarized')]='onlyPositiveWei\n",
       ">                          ghts',\n",
       ">                          scalingList:Union[list,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Compute weights of feature array X\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for whose rows conditional density estimates are computed. |\n",
       "| outputType | 'all' \\| 'onlyPositiveWeights' \\| 'summarized' \\| 'cumulativeDistribution' \\| 'cumulativeDistributionSummarized' | onlyPositiveWeights | Specifies structure of output. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SAA.getWeightsData\n",
       "\n",
       ">      SAA.getWeightsData (X:numpy.ndarray, outputType:Union[ForwardRef('all'),F\n",
       ">                          orwardRef('onlyPositiveWeights'),ForwardRef('summariz\n",
       ">                          ed'),ForwardRef('cumulativeDistribution'),ForwardRef(\n",
       ">                          'cumulativeDistributionSummarized')]='onlyPositiveWei\n",
       ">                          ghts',\n",
       ">                          scalingList:Union[list,numpy.ndarray,NoneType]=None)\n",
       "\n",
       "Compute weights of feature array X\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for whose rows conditional density estimates are computed. |\n",
       "| outputType | 'all' \\| 'onlyPositiveWeights' \\| 'summarized' \\| 'cumulativeDistribution' \\| 'cumulativeDistributionSummarized' | onlyPositiveWeights | Specifies structure of output. |\n",
       "| scalingList | list \\| np.ndarray \\| None | None | List or array with same size as self.Y containing floats being multiplied with self.Y. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SAA.getWeightsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
