{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class Weights-Based Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseWeightsBasedPredictor(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        \"\"\"Define weights-based predictor\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit weights-based predictor on given training data\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getWeightsData(self, X, scalingList = None):\n",
    "        \"\"\"Compute weights of feature array X\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self, X, probs = [0.1, 0.5, 0.9], outputAsDf = False, scalingList = None):\n",
    "        \n",
    "        distributionDataList = self.getWeightsData(X = X,\n",
    "                                                   outputType = 'cumulativeDistribution',\n",
    "                                                   scalingList = scalingList)\n",
    "        \n",
    "        quantilesDict = {prob: [] for prob in probs}\n",
    "        \n",
    "        for probsDistributionFunction, YDistributionFunction in distributionDataList:\n",
    "        \n",
    "            for prob in probs:\n",
    "                quantileIndex = np.where(probsDistributionFunction >= prob)[0][0]\n",
    "                quantile = YDistributionFunction[quantileIndex]\n",
    "                quantilesDict[prob].append(quantile)\n",
    "        \n",
    "        quantilesDf = pd.DataFrame(quantilesDict)\n",
    "        \n",
    "        # Just done to make the dictionary contain arrays rather than lists of the quantiles.\n",
    "        quantilesDict = {prob: np.array(quantiles) for prob, quantiles in quantilesDict.items()}\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if outputAsDf:\n",
    "            return quantilesDf\n",
    "        \n",
    "        else:\n",
    "            return quantilesDict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelSetForecaster(BaseWeightsBasedPredictor):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, \n",
    "                 binSize = None):\n",
    "        \n",
    "        if not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if not (isinstance(binSize, (int, np.integer)) or binSize is None):\n",
    "            raise ValueError(\"'binSize' has to be integer (or None if it is supposed to be tuned)!\")\n",
    "        else:\n",
    "            self.binSize = binSize\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.Y = None\n",
    "        self.YPred = None\n",
    "        self.binPerTrainPred = None\n",
    "        self.indicesPerBin = None\n",
    "        self.nearestNeighborsOnPreds = None\n",
    "        \n",
    "    #---        \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        YPredTrain = self.estimator.predict(X)\n",
    "        \n",
    "        binPerTrainPred, indicesPerBin = generateBins(binSize = self.binSize,\n",
    "                                                      YPredTrain = YPredTrain)\n",
    "\n",
    "        #---\n",
    "        \n",
    "        nn = NearestNeighbors(algorithm = 'kd_tree')\n",
    "        YPredTrain_reshaped = np.reshape(YPredTrain, newshape = (len(YPredTrain), 1))\n",
    "\n",
    "        nn.fit(X = YPredTrain_reshaped)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        self.Y = Y\n",
    "        self.YPredTrain = YPredTrain\n",
    "        self.binPerTrainPred = binPerTrainPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.nearestNeighborsOnPreds = nn\n",
    "        \n",
    "    #---\n",
    "        \n",
    "    def getWeightsData(self, X, outputType = 'onlyPositiveWeights', scalingList = None):\n",
    "        \n",
    "        binPerTrainPred = self.binPerTrainPred\n",
    "        indicesPerBin = self.indicesPerBin\n",
    "        nearestNeighborsOnPreds = self.nearestNeighborsOnPreds\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        YPred = self.estimator.predict(X)   \n",
    "        YPred_reshaped = np.reshape(YPred, newshape = (len(YPred), 1))\n",
    "        \n",
    "        nearestPredIndex = nearestNeighborsOnPreds.kneighbors(X = YPred_reshaped, \n",
    "                                                              n_neighbors = 1, \n",
    "                                                              return_distance = False).ravel()\n",
    "        \n",
    "        nearestPredNeighbor = self.YPredTrain[nearestPredIndex]\n",
    "\n",
    "        neighborsList = [indicesPerBin[binPerTrainPred[nearestPredNeighbor[i]]] for i in range(len(YPred))]\n",
    "\n",
    "        #---\n",
    "        \n",
    "        # Checks        \n",
    "        for i in range(len(neighborsList)):\n",
    "            if len(neighborsList[i]) < self.binSize:\n",
    "                ipdb.set_trace()\n",
    "\n",
    "        #---\n",
    "        \n",
    "        # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     Y = self.Y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBins(binSize, YPredTrain):\n",
    "    \n",
    "    \"\"\"\n",
    "    Used to generate the bin-structure induced by the Level-Set-Forecaster algorithm for\n",
    "    ``neighborStrategy == 'bins'``.\n",
    "    Bins are created by starting at the lowest value of YPredTrain and then succesively \n",
    "    adding the closest next prediction to the current bin until ``binSize``-many observations\n",
    "    have been allocated. Then the generation of a new bin is started in the same manner\n",
    "    until all values of YPredTrain have been assigned to exactly one bin.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    binSize : int\n",
    "        The size of each bin that is being created. Every bin is going to have this size\n",
    "        apart from the one containing the predictions with the highest values (see remarks)\n",
    "    YPredTrain : array\n",
    "        The predictions for the training observations.\n",
    "\n",
    "    Output\n",
    "    ----------\n",
    "    binPerPred : dict\n",
    "        A dictionary whose keys are given by all unique values of YPredTrain.\n",
    "        binPrePred[pred] returns the bin to which the current prediction 'pred'\n",
    "        belongs to.\n",
    "    indicesPerBin : dict\n",
    "        A dictionary whose keys are given by all bins (the keys begin at zero).\n",
    "        indicesPerBin[j] contains all indices of YPredTrain that belong to the same bin.\n",
    "\n",
    "    Notes\n",
    "    ----------\n",
    "    The binning strategy leads to the bin of the highest prediction values being smaller\n",
    "    than ``binSize``. As a convention, no bin is allowed to be smaller than ``binSize``.\n",
    "    For that reason, the bin of the highest value is joined with the next bin to it, \n",
    "    so the final bin containing the highest prediction values is the only bin to contain\n",
    "    more observations than ``binSize``.\n",
    "    \"\"\"\n",
    "    \n",
    "    YPredTrainUnique = pd.Series(YPredTrain).unique()\n",
    "    predIndicesSort = np.argsort(YPredTrainUnique)\n",
    "    \n",
    "    YPredTrainUniqueSorted = YPredTrainUnique[predIndicesSort]\n",
    "    indicesByPredTrain = [np.where(pred == YPredTrain)[0] for pred in YPredTrainUniqueSorted]\n",
    "    \n",
    "    currentBinSize = 0\n",
    "    binIndex = 0\n",
    "    binExisting = False\n",
    "    trainIndicesLeft = len(YPredTrain)\n",
    "    binPerPred = dict()\n",
    "    indicesPerBin = dict()\n",
    "\n",
    "    for i in range(len(indicesByPredTrain)):\n",
    "        currentBinSize += len(indicesByPredTrain[i])\n",
    "        binPerPred[YPredTrainUniqueSorted[i]] = binIndex\n",
    "        \n",
    "        if binExisting:\n",
    "            indicesPerBin[binIndex] = np.append(indicesPerBin[binIndex], indicesByPredTrain[i])\n",
    "        else:\n",
    "            indicesPerBin[binIndex] = indicesByPredTrain[i]\n",
    "            binExisting = True\n",
    "\n",
    "        trainIndicesLeft -= len(indicesByPredTrain[i])\n",
    "        if trainIndicesLeft < binSize:\n",
    "            for j in np.arange(i+1, len(indicesByPredTrain), 1):\n",
    "                binPerPred[YPredTrainUniqueSorted[j]] = binIndex\n",
    "                indicesPerBin[binIndex] = np.append(indicesPerBin[binIndex], indicesByPredTrain[j])\n",
    "            break\n",
    "\n",
    "        if currentBinSize >= binSize:\n",
    "            binIndex += 1\n",
    "            currentBinSize = 0\n",
    "            binExisting = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Checks\n",
    "    \n",
    "    indices = np.array([])\n",
    "    for k in range(len(indicesPerBin.keys())):\n",
    "        indices = np.append(indices, indicesPerBin[k])\n",
    " \n",
    "    if len(indices) != len(YPredTrain):\n",
    "        ipdb.set_trace()\n",
    "    \n",
    "    predCheck = np.array([pred in binPerPred.keys() for pred in YPredTrain])\n",
    "    keyCheck = np.array([key in YPredTrain for key in binPerPred.keys()])\n",
    "    \n",
    "    if (all(predCheck) & all(keyCheck)) is False:\n",
    "        ipdb.set_trace()\n",
    "    \n",
    "    return binPerPred, indicesPerBin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSF kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelSetForecaster_kNN(BaseWeightsBasedPredictor):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, \n",
    "                 binSize = 100):\n",
    "        \n",
    "        if not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if not isinstance(binSize, (int, np.integer)):\n",
    "            raise ValueError(\"'binSize' has to be integer!\")\n",
    "        else:\n",
    "            self.binSize = binSize\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.Y = None\n",
    "        self.YPred = None\n",
    "        self.nearestNeighborsOnPreds = None\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def setBinSize(self, binSize):\n",
    "        self.binSize = binSize\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        YPredTrain = self.estimator.predict(X)\n",
    "        YPredTrain_reshaped = np.reshape(YPredTrain, newshape = (len(YPredTrain), 1))\n",
    "        \n",
    "        nn = NearestNeighbors(algorithm = 'kd_tree')\n",
    "        nn.fit(X = YPredTrain_reshaped)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        self.Y = Y\n",
    "        self.YPredTrain = YPredTrain\n",
    "        self.nearestNeighborsOnPreds = nn\n",
    "        \n",
    "    #---\n",
    "        \n",
    "    def getWeightsData(self, X, outputType = 'onlyPositiveWeights', scalingList = None):\n",
    "        \n",
    "        nn = self.nearestNeighborsOnPreds\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        YPred = self.estimator.predict(X)   \n",
    "        YPred_reshaped = np.reshape(YPred, newshape = (len(YPred), 1))\n",
    "        \n",
    "        distancesDf, neighborsMatrix = nn.kneighbors(X = YPred_reshaped, \n",
    "                                                     n_neighbors = self.binSize + 1)\n",
    "        \n",
    "        #---\n",
    "\n",
    "        neighborsList = list(neighborsMatrix[:, 0:self.binSize])\n",
    "        distanceCheck = np.where(distancesDf[:, self.binSize - 1] == distancesDf[:, self.binSize])\n",
    "        indicesToMod = distanceCheck[0]\n",
    "\n",
    "        for index in indicesToMod:\n",
    "            distanceExtremePoint = np.absolute(YPred[index] - self.YPredTrain[neighborsMatrix[index, self.binSize-1]])\n",
    "            \n",
    "            neighborsByRadius = nn.radius_neighbors(X = YPred_reshaped[index:index + 1], \n",
    "                                                    radius = distanceExtremePoint, return_distance = False)[0]\n",
    "            neighborsList[index] = neighborsByRadius\n",
    "\n",
    "        #---\n",
    "        \n",
    "        for i in range(len(neighborsList)):\n",
    "            if len(neighborsList[i]) < self.binSize:\n",
    "                ipdb.set_trace()\n",
    "\n",
    "        #---\n",
    "                        \n",
    "        # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     Y = self.Y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSF Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binSizeCV:\n",
    "\n",
    "    def __init__(self,\n",
    "                 estimator,\n",
    "                 cv,\n",
    "                 LSF_type = None,\n",
    "                 binSizeGrid = np.array([4, 7, 10, 15, 20, 30, 40, 50, 60, 70, 80, \n",
    "                                         100, 125, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900,\n",
    "                                         1000, 1250, 1500, 1750, 2000, 2500, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]),                 \n",
    "                 probs = np.array([i / 100 for i in range(1, 100, 1)]),\n",
    "                 refitPerProb = False,\n",
    "                 n_jobs = None):\n",
    "        \n",
    "        # CHECKS\n",
    "        \n",
    "        if isinstance(estimator, (LevelSetForecaster, LevelSetForecaster_kNN)):\n",
    "            raise ValueError(\"'estimator' has to be a point predictor and not a LevelSetForecaster-Object!\")   \n",
    "        elif not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if LSF_type is None or not LSF_type in [\"LSF\", \"LSF_kNN\"]:\n",
    "            raise ValueError(\"LSF_type must be specified and must either be 'LSF' or 'LSF_kNN'!\")\n",
    "        else:\n",
    "            self.LSF_type = LSF_type\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # Ensure that binSizeGrid is an array if it is given by the user\n",
    "        binSizeGrid = np.array(binSizeGrid) \n",
    "        self.binSizeGrid = binSizeGrid\n",
    "        \n",
    "        self.probs = probs\n",
    "        self.cv = cv\n",
    "        self.refitPerProb = refitPerProb\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.best_binSize = None\n",
    "        self.best_binSize_perProb = None\n",
    "        self.best_EstimatorLSF = None\n",
    "        self.cv_results = None\n",
    "        self.cv_results_raw = None\n",
    "        \n",
    "    #---\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function. Creates weights-output by specifying considered\n",
    "    neighbors of training observations for every test observation of interest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    LSF : LSF-object\n",
    "        A object of that type is instantiated for each grid point. This is assumed \n",
    "        to implement the scikit-learn estimator interface. Either estimator needs to \n",
    "        provide a score function, or scoring must be passed.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "    \n",
    "        scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(scoresForFold)(binSizeGrid = self.binSizeGrid,\n",
    "                                                                              probs = self.probs,\n",
    "                                                                              foldNumber = foldNumber,\n",
    "                                                                              cvFolds = self.cv,\n",
    "                                                                              estimator = self.estimator,\n",
    "                                                                              LSF_type = self.LSF_type,\n",
    "                                                                              Y = Y,\n",
    "                                                                              X = X) for foldNumber in range(len(cvFolds)))    \n",
    "        \n",
    "        self.cv_results_raw = scoresPerFold\n",
    "        \n",
    "        #---\n",
    "\n",
    "        nvCostsMatrix = scoresPerFold[0]\n",
    "\n",
    "        for i in range(1, len(scoresPerFold)):\n",
    "            nvCostsMatrix = nvCostsMatrix + scoresPerFold[i]\n",
    "\n",
    "        nvCostsMatrix = nvCostsMatrix / len(cvFolds)\n",
    "        \n",
    "        self.cv_results = nvCostsMatrix\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        meanCostsDf = nvCostsMatrix.mean(axis = 1)\n",
    "        binSizeBestOverall = meanCostsDf.index[np.argmax(meanCostsDf)]\n",
    "        self.best_binSize = binSizeBestOverall\n",
    "        \n",
    "        binSizeBestPerProb = nvCostsMatrix.idxmax(axis = 0)\n",
    "        self.best_binSize_perProb = binSizeBestPerProb\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if self.refitPerProb:\n",
    "            \n",
    "            LSFDict = dict()\n",
    "            for binSize in binSizeBestPerProb.unique():\n",
    "                \n",
    "                if self.LSF_type == 'LSF':\n",
    "                    LSF = LevelSetForecaster(estimator = self.estimator, \n",
    "                                             binSize = binSize)\n",
    "                else:\n",
    "                    LSF = LevelSetForecaster_kNN(estimator = self.estimator, \n",
    "                                                 binSize = binSize)\n",
    "                \n",
    "                LSF.fit(X = X, Y = Y)\n",
    "                LSFDict[binSize] = LSF\n",
    "            \n",
    "            self.best_estimatorLSF = {prob: LSFDict[binSizeBestPerProb.loc[prob]] \n",
    "                                      for prob in binSizeBestPerProb.index}\n",
    "        \n",
    "        else:\n",
    "            if self.LSF_type == 'LSF':\n",
    "                LSF = LevelSetForecaster(estimator = self.estimator, \n",
    "                                         binSize = binSizeBestOverall)\n",
    "            else:\n",
    "                LSF = LevelSetForecaster_kNN(estimator = self.estimator, \n",
    "                                             binSize = binSizeBestOverall)\n",
    "            \n",
    "            LSF.fit(X = X, Y = Y)\n",
    "            \n",
    "            self.best_estimatorLSF = LSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def scoresForFold(binSizeGrid, probs, foldNumber, cvFolds, estimator, LSF_type, Y, X):\n",
    "   \n",
    "    indicesTrain = cvFolds[foldNumber][0]\n",
    "    indicesTest = cvFolds[foldNumber][1]\n",
    "    \n",
    "    YTrainFold = Y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    YTestFold = Y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    estimator.fit(X = XTrainFold, y = YTrainFold)\n",
    "    \n",
    "    #---\n",
    "       \n",
    "    SAA_fold = SAA()\n",
    "    SAA_fold.fit(Y = YTrainFold)\n",
    "    \n",
    "    # By setting 'X = None', the SAA results are only computed for a single observation (they are independent of X anyway).\n",
    "    # In order to receive the final dataframe of SAA results, we simply duplicate this single row as many times as needed.\n",
    "    quantilesDictSAAOneOb = SAA_fold.predict(X = None, probs = probs, outputAsDf = False)\n",
    "    quantilesDictSAA = {prob: np.repeat(quantile, len(XTestFold)) for prob, quantile in quantilesDictSAAOneOb.items()}\n",
    "    \n",
    "    #---\n",
    "                                                   \n",
    "    coefPresPerBinSize = list()\n",
    "    \n",
    "    binSizeGrid = [binSize for binSize in binSizeGrid if binSize <= len(YTrainFold)]\n",
    "    \n",
    "    for binSize in iter(binSizeGrid):\n",
    "        \n",
    "        if LSF_type == 'LSF':\n",
    "            estimatorLSF = LevelSetForecaster(estimator = estimator,\n",
    "                                              binSize = binSize)\n",
    "        else:\n",
    "            estimatorLSF = LevelSetForecaster_kNN(estimator = estimator,\n",
    "                                                  binSize = binSize)\n",
    "        \n",
    "        estimatorLSF.fit(X = XTrainFold,\n",
    "                         Y = YTrainFold)\n",
    "        \n",
    "        quantilesDict = estimatorLSF.predict(X = XTestFold,\n",
    "                                             probs = probs,\n",
    "                                             outputAsDf = False)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # coeffPres = Coefficient of Prescriptiveness\n",
    "        \n",
    "        coefPresDict = {prob: [] for prob in probs}\n",
    "        \n",
    "        for prob in probs:            \n",
    "            coefPres = getCoefPres(decisions = quantilesDict[prob], \n",
    "                                   decisionsSAA = quantilesDictSAA[prob], \n",
    "                                   YTest = YTestFold, \n",
    "                                   prob = prob)\n",
    "            \n",
    "            coefPresDict[prob].append(coefPres)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    coefPresDf = pd.DataFrame(coefPresDict, index = binSizeGrid)\n",
    "    \n",
    "    return coefPresDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Prescriptiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCoefPres(decisions, decisionsSAA, YTest, prob):\n",
    "\n",
    "    # Newsvendor Costs of our model\n",
    "    cost = np.array([prob * (YTest[i] - decisions[i]) if YTest[i] > decisions[i] \n",
    "                     else (1 - prob) * (decisions[i] - YTest[i]) \n",
    "                     for i in range(len(YTest))]).sum()\n",
    "    \n",
    "    # Newsvendor Costs of SAA\n",
    "    costSAA = np.array([prob * (YTest[i] - decisionsSAA[i]) if YTest[i] > decisionsSAA[i] \n",
    "                        else (1 - prob) * (decisionsSAA[i] - YTest[i]) \n",
    "                        for i in range(len(YTest))]).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # We have to capture the special case of costSAA == 0, because then we can't compute the \n",
    "    # Coefficient of Prescriptiveness using the actual definition.\n",
    "    if costSAA > 0:\n",
    "        coefPres = 1 - cost / costSAA\n",
    "    else:\n",
    "        if cost == 0:\n",
    "            coefPres = 1\n",
    "        else:\n",
    "            coefPres = 0\n",
    "    \n",
    "    return coefPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wSAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestWSAA(RandomForestRegressor, BaseWeightsBasedPredictor):\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        super(RandomForestRegressor, self).fit(X = X, y = Y)\n",
    "        \n",
    "        self.Y = Y\n",
    "        self.leafIndicesTrain = self.apply(X)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeightsData(self, X, outputType = 'onlyPositiveWeights', scalingList = None):\n",
    "        \n",
    "        leafIndicesDf = self.apply(X)\n",
    "        \n",
    "        weightsDataList = list()\n",
    "        \n",
    "        for leafIndices in leafIndicesDf:\n",
    "            leafComparisonMatrix = (self.leafIndicesTrain == leafIndices) * 1\n",
    "            nObsInSameLeaf = np.sum(leafComparisonMatrix, axis = 0)\n",
    "\n",
    "            # It can happen that RF decides that the best strategy is to fit no tree at\n",
    "            # all and simply average all results (happens when min_child_sample is too high, for example).\n",
    "            # In this case 'leafComparisonMatrix' mustn't be averaged because there has been only a single tree.\n",
    "            if len(leafComparisonMatrix.shape) == 1:\n",
    "                weights = leafComparisonMatrix / nObsInSameLeaf\n",
    "            else:\n",
    "                weights = np.mean(leafComparisonMatrix / nObsInSameLeaf, axis = 1)\n",
    "            \n",
    "            weightsPosIndex = np.where(weights > 0)[0]\n",
    "            \n",
    "            weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     Y = self.Y, \n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = False)\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self, X, probs = np.array([0.1, 0.5, 0.9]), outputAsDf = False, scalingList = None):\n",
    "        \n",
    "        quantileRes = super(BaseWeightsBasedPredictor, self).predict(X = X,\n",
    "                                                                     probs = probs,\n",
    "                                                                     outputAsDf = outputAsDf,\n",
    "                                                                     scalingList = scalingList)\n",
    "        \n",
    "        return quantileRes\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self, X):\n",
    "        \n",
    "        preds = super(RandomForestRegressor, self).predict(X)\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAA(BaseWeightsBasedPredictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Y = None\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeightsData(self, X = None, outputType = 'onlyPositiveWeights', scalingList = None):\n",
    "        \n",
    "        if X is None:\n",
    "            neighborsList = [np.arange(len(self.Y))]\n",
    "        else:\n",
    "            neighborsList = [np.arange(len(self.Y)) for i in range(X.shape[0])]\n",
    "        \n",
    "        # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     Y = self.Y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Weights Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructureWeightsDataList(weightsDataList, outputType = 'onlyPositiveWeights', Y = None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function. Creates weights-output by specifying considered\n",
    "    neighbors of training observations for every test observation of interest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neighborsList : {list}\n",
    "        The i-th list-entry is supposed to correspond to the i-th test observation. \n",
    "        Every list-entry should be a array containing the indices of training observations\n",
    "        which were selected as the neighbors of the considered test observation based on\n",
    "        the selected Level-Set-Forecaster algorithm.     \n",
    "    outputType : {\"summarized\", \"onlyPositiveWeights\", \"all\"}, default=\"onlyPositiveWeights\"\n",
    "        Specifies the structure of the output. \n",
    "        - If \"all\", then the weights are outputted as an array that is exactly as long as \n",
    "          the number of training observations. Consequently, also weights equal to zero are\n",
    "          being computed. \n",
    "          NOTE: This can be take up lots of RAM for large datasets with\n",
    "          > 10^6 observations.\n",
    "        - If \"onlyPositiveWeights\", then weights equal to zero are truncated. In order to be \n",
    "          able to identify to which training observation each weight belongs, a tuple is\n",
    "          outputted whose first entry are the weights and the second one are the corresponding\n",
    "          training indices. \n",
    "        - If \"summarized\", then additionally to \"onlyPositiveWeights\", weights referencing to the\n",
    "          same y-value are condensed to one single weight. In this case, the second entry of the\n",
    "          outputted tuple contains the y-values to which each weight corresponds. \n",
    "          NOTE: Summarizing the weights can be very computationally burdensome if roughly the considered\n",
    "          dataset has more than 10^6 observations and if ``binSize`` > 10^4.\n",
    "        - If \"cumulativeDistributionSummarized\", then additionally to \"summarized\", the cumulative sum of the\n",
    "          weights is computed, which can be interpreted as the empirical cumulative distribution\n",
    "          function given the feature vector at hand.\n",
    "          NOTE: This output type requires summarizing the weights, which can be very computationally \n",
    "          burdensome if roughly the considered dataset has more than 10^6 observations and if \n",
    "          ``binSize`` > 10^4.\n",
    "    Y: array, default=None\n",
    "        The target values of the training observations. Only needed when ``outputType`` is given as \n",
    "        \"all\" or \"summarized\".\"\"\"\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(Y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, YUnique = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                              YWeightPos = YWeightPos,\n",
    "                                                              equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YUnique = YUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, YUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistribution':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            indicesSort = np.argsort(YWeightPos)\n",
    "            \n",
    "            weightsPosSorted = weightsPos[indicesSort]\n",
    "            YWeightPosSorted = YWeightPos[indicesSort]\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsPosSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YWeightPosSorted = YWeightPosSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, YWeightPosSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistributionSummarized':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarizedSorted, YPosWeightUniqueSorted = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                                                   YWeightPos = YWeightPos,\n",
    "                                                                                   equalWeights = equalWeights)\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsSummarizedSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YPosWeightUniqueSorted = YPosWeightUniqueSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, YPosWeightUniqueSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Weights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeWeightsData(weightsPos, YWeightPos, equalWeights = False):\n",
    "    \n",
    "    if equalWeights:\n",
    "        counterDict = Counter(YWeightPos)\n",
    "        YUniqueSorted = np.sort(list(counterDict.keys()))\n",
    "\n",
    "        weightsSummarizedSorted = np.array([counterDict[value] / len(YWeightPos) for value in YUniqueSorted])\n",
    "    \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "\n",
    "        for i, item in enumerate(YWeightPos):\n",
    "            duplicationDict[item].append(i)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        YUnique = list()\n",
    "\n",
    "        for value, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            YUnique.append(value)\n",
    "\n",
    "        weightsSummarized, YUnique = np.array(weightsSummarized), np.array(YUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        indicesSort = np.argsort(YUnique)\n",
    "        weightsSummarizedSorted, YUniqueSorted = weightsSummarized[indicesSort], YUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, YUniqueSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFinalOutput(dataOriginal, \n",
    "                        dataDecisions, \n",
    "                        demandFeature = 'actuals', \n",
    "                        mergeOn = None, \n",
    "                        variablesToAdd = None, \n",
    "                        rescaleDemandFeature = False, \n",
    "                        includeTraining = False, \n",
    "                        sortBy = None,\n",
    "                        longFormat = False,\n",
    "                        **kwargs):\n",
    "    \n",
    "    dataOriginal = dataOriginal.rename(columns = {demandFeature: 'actuals'})\n",
    "    \n",
    "    if rescaleDemandFeature:\n",
    "        if 'scalingValue' in dataOriginal.columns:\n",
    "            dataOriginal['actuals'] = dataOriginal['actuals'] * dataOriginal['scalingValue']\n",
    "        else:\n",
    "            raise ValueError(\"If 'rescaleDemandFeature' is set as 'True', a column\"\n",
    "                             \"named 'scalingValue' has to be part of 'data!'\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Adding additional data to the matrix dataDecisions\n",
    "    # NOTE: This function is written to be useable for all datasets we currently use. \n",
    "    colsToAdd = ['id', 'sku_code_prefix', 'sku_code', 'SKU_API',\n",
    "                 'actuals', 'revenue', 'label',\n",
    "                 'adi', 'adi_sku', 'adi_product', 'cv2', 'cvDemand_sku', 'cvDemand_product']\n",
    "    \n",
    "    if isinstance(mergeOn, list):\n",
    "        colsToAdd = colsToAdd + mergeOn\n",
    "    else:\n",
    "        colsToAdd.append(mergeOn)\n",
    "    \n",
    "    if isinstance(variablesToAdd, list):\n",
    "        colsToAdd = colsToAdd + variablesToAdd\n",
    "    else:\n",
    "        colsToAdd.append(variablesToAdd)\n",
    "        \n",
    "    if isinstance(sortBy, list):\n",
    "        colsToAdd = colsToAdd + sortBy\n",
    "    else:\n",
    "        colsToAdd.append(sortBy)\n",
    "    \n",
    "    colsToAdd = pd.Series(colsToAdd)\n",
    "    colsToAdd = colsToAdd[colsToAdd.isin(dataOriginal.columns)] \n",
    "    colsToAdd = colsToAdd.unique()\n",
    "    \n",
    "    dataTestInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'test', colsToAdd].reset_index(drop = True)\n",
    "    \n",
    "    if not longFormat:\n",
    "\n",
    "        if mergeOn is None:\n",
    "            dataResults = pd.concat([dataTestInfoToAdd, dataDecisions.reset_index(drop = True)], axis = 1)\n",
    "        else:\n",
    "            dataResults = pd.merge(dataTestInfoToAdd, dataDecisions, on = mergeOn)\n",
    "\n",
    "        #---\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dataDecisionsStacked = dataDecisions.stack().reset_index().set_index('level_0')\n",
    "        dataDecisionsStacked.rename(columns = {'level_1': 'decisionType'}, inplace = True)\n",
    "        \n",
    "        dataDecisionsStacked.rename(columns = {0: 'decisions'}, inplace = True)\n",
    "        dataDecisionsStacked.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        numberOfDecisionTypes = len(dataDecisionsStacked['decisionType'].unique())\n",
    "        \n",
    "        infoDuplicatedDf = dataTestInfoToAdd.loc[dataTestInfoToAdd.index.repeat(numberOfDecisionTypes)]\n",
    "        infoDuplicatedDf.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        dataResults = pd.concat([infoDuplicatedDf, dataDecisionsStacked], axis = 1)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    infoToAdd = pd.DataFrame(kwargs, index = [0])\n",
    "    infoToAdd['label'] = 'test'\n",
    "    dataResults = pd.merge(dataResults, infoToAdd, on = 'label')\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if includeTraining:\n",
    "        dataTrainInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'train', colsToAdd].reset_index(drop = True)\n",
    "        dataResults = pd.concat([dataTrainInfoToAdd, dataResults], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if not sortBy is None:\n",
    "        \n",
    "        if not all([sortByCol in dataResults.columns for sortByCol in sortBy]):\n",
    "            raise ValueError(\"Columns specified by 'sortBy' must be part of 'dataOriginal'!\")\n",
    "        else:\n",
    "            dataResults.sort_values(by = sortBy, axis = 0, inplace = True, ignore_index = True)\n",
    "    \n",
    "    return dataResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
