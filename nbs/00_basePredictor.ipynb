{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp basePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class - Weights-Based Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasePredictor(ABC):\n",
    "    \"\"\" \n",
    "    Base class that implements the 'prediction'-method for approaches based \n",
    "    on a reweighting of the empirical distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        \"\"\"Define weights-based predictor\"\"\"\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit weights-based predictor on training samples\"\"\"\n",
    "        \n",
    "    def getWeightsData(self, X):\n",
    "        \"\"\"Compute weights for every sample specified by feature matrix 'X'\"\"\"\n",
    "\n",
    "    def predictQuantiles(self: BasePredictor, \n",
    "                         X: np.ndarray, # Feature matrix of samples for which conditional quantiles are computed.\n",
    "                         probs: list | np.ndarray = [0.1, 0.5, 0.9], # Probabilities for which the estimated conditional p-quantiles are computed.\n",
    "                         outputAsDf: bool = False, # Output is either a dataframe with 'probs' as cols or a dict with 'probs' as keys.\n",
    "                         scalingList: list | np.ndarray | None = None, # List or array with same size as self.Y containing floats being multiplied with self.Y.\n",
    "                         ):\n",
    "\n",
    "        distributionDataList = self.getWeightsData(X = X,\n",
    "                                                   outputType = 'cumulativeDistribution',\n",
    "                                                   scalingList = scalingList)\n",
    "\n",
    "        quantilesDict = {prob: [] for prob in probs}\n",
    "\n",
    "        for probsDistributionFunction, yDistributionFunction in distributionDataList:\n",
    "\n",
    "            for prob in probs:\n",
    "                quantileIndex = np.where(probsDistributionFunction >= prob)[0][0]\n",
    "                quantile = yDistributionFunction[quantileIndex]\n",
    "                quantilesDict[prob].append(quantile)\n",
    "\n",
    "        quantilesDf = pd.DataFrame(quantilesDict)\n",
    "\n",
    "        # Just done to make the dictionary contain arrays rather than lists of the quantiles.\n",
    "        quantilesDict = {prob: np.array(quantiles) for prob, quantiles in quantilesDict.items()}\n",
    "\n",
    "        #---\n",
    "\n",
    "        if outputAsDf:\n",
    "            return quantilesDf\n",
    "\n",
    "        else:\n",
    "            return quantilesDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/basePredictor.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasePredictor\n",
       "\n",
       ">      BasePredictor ()\n",
       "\n",
       "Base class that implements the 'prediction'-method for approaches based \n",
       "on a reweighting of the empirical distribution."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/basePredictor.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasePredictor\n",
       "\n",
       ">      BasePredictor ()\n",
       "\n",
       "Base class that implements the 'prediction'-method for approaches based \n",
       "on a reweighting of the empirical distribution."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BasePredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseWeightsBasedPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBasePredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictQuantiles\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/nbdev/showdoc.py:191\u001b[0m, in \u001b[0;36mshow_doc\u001b[0;34m(sym, renderer, name, title_level)\u001b[0m\n\u001b[1;32m    189\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(import_module(p), m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sym, TypeDispatch): \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msym\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshow_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_level\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/nbdev/showdoc.py:127\u001b[0m, in \u001b[0;36mShowDocRenderer.__init__\u001b[0;34m(self, sym, name, title_level)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m qual_name(sym)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misfunc \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misfunction(sym)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msig \u001b[38;5;241m=\u001b[39m \u001b[43msignature_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43msym\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m,\u001b[38;5;167;01mTypeError\u001b[39;00m): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs \u001b[38;5;241m=\u001b[39m _docstring(sym)\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/basics.py:350\u001b[0m, in \u001b[0;36msignature_ex\u001b[0;34m(obj, eval_str)\u001b[0m\n\u001b[1;32m    348\u001b[0m sig \u001b[38;5;241m=\u001b[39m signature(obj)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m ann \u001b[38;5;241m=\u001b[39m \u001b[43mtype_hints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m params \u001b[38;5;241m=\u001b[39m [_eval_param(ann,k,v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Signature(params, return_annotation\u001b[38;5;241m=\u001b[39msig\u001b[38;5;241m.\u001b[39mreturn_annotation)\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/basics.py:318\u001b[0m, in \u001b[0;36mtype_hints\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, typing\u001b[38;5;241m.\u001b[39m_allowed_types): \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m    317\u001b[0m ann,glb,loc \u001b[38;5;241m=\u001b[39m get_annotations_ex(f)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k:_eval_type(v,glb,loc) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m ann\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/basics.py:318\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, typing\u001b[38;5;241m.\u001b[39m_allowed_types): \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m    317\u001b[0m ann,glb,loc \u001b[38;5;241m=\u001b[39m get_annotations_ex(f)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k:\u001b[43m_eval_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m ann\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/basics.py:311\u001b[0m, in \u001b[0;36m_eval_type\u001b[0;34m(t, glb, loc)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_eval_type\u001b[39m(t, glb, loc):\n\u001b[0;32m--> 311\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43meval_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NoneType \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m res\n",
      "File \u001b[0;32m~/.conda/envs/HC-Scheduling/lib/python3.8/site-packages/fastcore/basics.py:305\u001b[0m, in \u001b[0;36meval_type\u001b[0;34m(t, glb, loc)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t,\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m t: \u001b[38;5;28;01mreturn\u001b[39;00m Union[eval_type(\u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)), glb, loc)]\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t,(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;28mlist\u001b[39m)): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t)([eval_type(c, glb, loc) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m t])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseWeightsBasedPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "show_doc(BasePredictor.predictQuantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure Weights Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def restructureWeightsDataList(weightsDataList, outputType = 'onlyPositiveWeights', y= None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function. Creates weights-output by specifying considered\n",
    "    neighbors of training observations for every test observation of interest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neighborsList : {list}\n",
    "        The i-th list-entry is supposed to correspond to the i-th test observation. \n",
    "        Every list-entry should be a array containing the indices of training observations\n",
    "        which were selected as the neighbors of the considered test observation based on\n",
    "        the selected Level-Set-Forecaster algorithm.     \n",
    "    outputType : {\"summarized\", \"onlyPositiveWeights\", \"all\"}, default=\"onlyPositiveWeights\"\n",
    "        Specifies the structure of the output. \n",
    "        - If \"all\", then the weights are outputted as an array that is exactly as long as \n",
    "          the number of training observations. Consequently, also weights equal to zero are\n",
    "          being computed. \n",
    "          NOTE: This can be take up lots of RAM for large datasets with\n",
    "          > 10^6 observations.\n",
    "        - If \"onlyPositiveWeights\", then weights equal to zero are truncated. In order to be \n",
    "          able to identify to which training observation each weight belongs, a tuple is\n",
    "          outputted whose first entry are the weights and the second one are the corresponding\n",
    "          training indices. \n",
    "        - If \"summarized\", then additionally to \"onlyPositiveWeights\", weights referencing to the\n",
    "          same y-value are condensed to one single weight. In this case, the second entry of the\n",
    "          outputted tuple contains the y-values to which each weight corresponds. \n",
    "          NOTE: Summarizing the weights can be very computationally burdensome if roughly the considered\n",
    "          dataset has more than 10^6 observations and if ``binSize`` > 10^4.\n",
    "        - If \"cumulativeDistributionSummarized\", then additionally to \"summarized\", the cumulative sum of the\n",
    "          weights is computed, which can be interpreted as the empirical cumulative distribution\n",
    "          function given the feature vector at hand.\n",
    "          NOTE: This output type requires summarizing the weights, which can be very computationally \n",
    "          burdensome if roughly the considered dataset has more than 10^6 observations and if \n",
    "          ``binSize`` > 10^4.\n",
    "    y: array, default=None\n",
    "        The target values of the training observations. Only needed when ``outputType`` is given as \n",
    "        \"all\" or \"summarized\".\"\"\"\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, yUnique = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                              yWeightPos = yWeightPos,\n",
    "                                                              equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yUnique = yUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, yUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistribution':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            indicesSort = np.argsort(yWeightPos)\n",
    "            \n",
    "            weightsPosSorted = weightsPos[indicesSort]\n",
    "            yWeightPosSorted = yWeightPos[indicesSort]\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsPosSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yWeightPosSorted = yWeightPosSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, yWeightPosSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistributionSummarized':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarizedSorted, yPosWeightUniqueSorted = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                                                   yWeightPos = yWeightPos,\n",
    "                                                                                   equalWeights = equalWeights)\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsSummarizedSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yPosWeightUniqueSorted = yPosWeightUniqueSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, yPosWeightUniqueSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize Weights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarizeWeightsData(weightsPos, yWeightPos, equalWeights = False):\n",
    "    \n",
    "    if equalWeights:\n",
    "        counterDict = Counter(yWeightPos)\n",
    "        yUniqueSorted = np.sort(list(counterDict.keys()))\n",
    "\n",
    "        weightsSummarizedSorted = np.array([counterDict[value] / len(yWeightPos) for value in yUniqueSorted])\n",
    "    \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "\n",
    "        for i, item in enumerate(yWeightPos):\n",
    "            duplicationDict[item].append(i)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        yUnique = list()\n",
    "\n",
    "        for value, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            yUnique.append(value)\n",
    "\n",
    "        weightsSummarized, yUnique = np.array(weightsSummarized), np.array(yUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        indicesSort = np.argsort(yUnique)\n",
    "        weightsSummarizedSorted, yUniqueSorted = weightsSummarized[indicesSort], yUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, yUniqueSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
