package:
  name: dddex
  version: 0.0.9
source:
  sha256: 9d8ad9cf7a98f905e290f4ca5e087a325a97df5ea256010f06d122d28986e62a
  url: https://files.pythonhosted.org/packages/b0/1e/af1fc4bedca758a8c70c13cab0b2e9dd5279c81fc106fe95628369a8569a/dddex-0.0.9.tar.gz
about:
  description: "dddex: Data-Driven Density Estimation x\n================\n\n<!--\
    \ WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n## Install\n\n```\
    \ sh\npip install dddex\n```\n\n## What is dddex?\n\nThe package name `dddex`\
    \ stands for *Data-Driven Density Estimaton x*.\nNew approaches are being implemented\
    \ for estimating conditional\ndensities without any parametric assumption about\
    \ the underlying\ndistribution. All those approaches take an arbitrary point forecaster\
    \ as\ninput and turn them into a new object that outputs an estimation of the\n\
    conditional density based on the point predictions of the original point\nforecaster.\
    \ The *x* in the name emphasizes that the approaches can be\napplied to any point\
    \ forecaster. The approaches are being implemented\nvia the two classes\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    and\n[`LevelSetKDEx_kNN`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex_knn)\n\
    whose usage is explained in detail below.\n\nAll models can be run easily with\
    \ only a few lines of code and are\ndesigned to be compatible with the well known\
    \ *Scikit-Learn* framework.\n\n## How to use: LevelSetKDEx\n\nTo ensure compatibility\
    \ with Scikit-Learn, as usual the classes\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    and\n[`LevelSetKDEx_kNN`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex_knn)\n\
    both implement a `fit` and `predict` method. As the purposes of both\nclasses\
    \ is to compute estimations of conditional densities, the\n`predict` method outputs\
    \ p-quantiles rather than point forecasts.\n\nOur choice of the class-names is\
    \ supposed to be indicative of the\nunderlying models: The name *LevelSet* stems\
    \ from the fact that both\nmethods operate with the underlying assumption that\
    \ the values of point\nforecasts generated by the same point forecaster can be\
    \ interpreted as a\nsimilarity measure between samples. *KDE* is short for *Kernel\
    \ Density\nEstimator* and the *x* yet again signals that the classes can be\n\
    initialized with any point forecasting model.\n\nIn the following, we demonstrate\
    \ how to use the class\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    to compute estimations of the conditional densities and quantiles for\nthe [Yaz\
    \ Data\nSet](https://opimwue.github.io/ddop/modules/auto_generated/ddop.datasets.load_yaz.html#ddop.datasets.load_yaz).\n\
    As explained above,\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    is always based on a point forecaster that is being specified by the\nuser. In\
    \ our example we use the well known `LightGBMRegressor` as the\nunderlying point\
    \ predictor.\n\n``` python\nfrom lightgbm import LGBMRegressor\nfrom dddex.levelSetKDEx\
    \ import LevelSetKDEx\nfrom dddex.loadData import loadDataYaz\n```\n\n``` python\n\
    dataYaz, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 28, returnXY =\
    \ True)\nLGBM = LGBMRegressor(n_jobs = 1)\n```\n\nThere are three parameters for\n\
    [`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex):\n\
    \n- **estimator**: A point forecasting model that must have a `predict`\n  method.\n\
    - **binSize**: The amount of training samples considered to compute the\n  conditional\
    \ densities (for more details, see *To be written*).\n- **weightsByDistance**:\
    \ If *False*, all considered training samples are\n  weighted equally. If *True*,\
    \ training samples are weighted by the\n  inverse of the distance of their respective\
    \ point forecast to the\n  point forecast of the test sample at hand.\n\n``` python\n\
    LSKDEx = LevelSetKDEx(estimator = LGBM, \n                      binSize = 100,\n\
    \                      weightsByDistance = False)\n```\n\nThere is no need to\
    \ run `fit` on the point forecasting model before\ninitializing *LevelSetKDEx*,\
    \ because the `fit` method of\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    automatically checks whether the provided model has been fitted already\nor not\
    \ and runs the respective `fit` method of the point forecaster if\nneeded.\n\n\
    It should be noted, that running `fit` for the *LevelSetKDEx* approaches\ntakes\
    \ exceptionally little time even for datasets with $>10^6$ samples\n(provided,\
    \ of course, that the underlying point forecasting model has\nbeen fitted before\
    \ hand).\n\n``` python\nLSKDEx.fit(X = XTrain, y = yTrain)\n```\n\nIn order to\
    \ compute conditional densities for test samples now, we\nsimply run the `getWeights`\
    \ method.\n\n``` python\nconditionalDensities = LSKDEx.getWeights(X = XTest,\n\
    \                                         outputType = 'summarized')\n\nprint(f\"\
    probabilities: {conditionalDensities[0][0]}\")\nprint(f\"demand values: {conditionalDensities[0][1]}\"\
    )\n```\n\n    probabilities: [0.49 0.01 0.21 0.01 0.16 0.07 0.04 0.01]\n    demand\
    \ values: [0.         0.01075269 0.04       0.04878049 0.08       0.12\n     0.16\
    \       0.2       ]\n\nHere, `conditionalDensities` is a list whose elements correspond\
    \ to the\nsamples specified via `X`. Every element contains a tuple, whose first\n\
    entry constitutes probabilities and the second entry corresponding\ndemand values\
    \ (side note: The demand values have been scaled to lie in\n$[0, 1]$). In the\
    \ above example, we can for example see that our model\nestimates that for the\
    \ first test sample the demand will be 0 with a\nprobability of 49%.\n\nLike the\
    \ input argument *outputType* of `getWeights` suggests, we can\noutput the conditional\
    \ density estimations in various different forms.\nAll in all, there are currently\
    \ 5 output types specifying how the output\nfor each sample looks like:\n\n- **all**:\
    \ An array with the same length as the number of training\n  samples. Each entry\
    \ represents the probability of each training\n  sample.\n- **onlyPositiveWeights**:\
    \ A tuple. The first element of the tuple\n  represents the probabilities and\
    \ the second one the indices of the\n  corresponding training sample. Only probalities\
    \ greater than zero are\n  returned. Note: This is the most memory and computationally\
    \ efficient\n  output type.\n- **summarized**: A tuple. The first element of the\
    \ tuple represents the\n  probabilities and the second one the corresponding value\
    \ of `yTrain`.\n  The probabilities corresponding to identical values of `yTrain`\
    \ are\n  aggregated.\n- **cumulativeDistribution**: A tuple. The first element\
    \ of the tuple\n  represents the probabilities and the second one the corresponding\n\
    \  value of `yTrain`.\n- **cumulativeDistributionSummarized**: A tuple. The first\
    \ element of\n  the tuple represents the probabilities and the second one the\n\
    \  corresponding value of `yTrain`. The probabilities corresponding to\n  identical\
    \ values of `yTrain` are aggregated.\n\nFor example, by setting\n`outputType =\
    \ 'cumulativeDistributionSummarized'` we can compute an\nestimation of the conditional\
    \ cumulative distribution function for each\nsample. Below, we can see that our\
    \ model predicts the demand of the\nfirst sample to be lower or equal than 0.16\
    \ with a probability of 99%.\n\n``` python\ncumulativeDistributions = LSKDEx.getWeights(X\
    \ = XTest,\n                                            outputType = 'cumulativeDistributionSummarized')\n\
    \nprint(f\"cumulated probabilities: {cumulativeDistributions[0][0]}\")\nprint(f\"\
    demand values: {cumulativeDistributions[0][1]}\")\n```\n\n    cumulated probabilities:\
    \ [0.49 0.5  0.71 0.72 0.88 0.95 0.99 1.  ]\n    demand values: [0.         0.01075269\
    \ 0.04       0.04878049 0.08       0.12\n     0.16       0.2       ]\n\nWe can\
    \ also compute estimations of quantiles using the `predict` method.\nThe parameter\
    \ *probs* specifies the quantiles we want to predict.\n\n``` python\npredRes =\
    \ LSKDEx.predict(X = XTest,\n                         outputAsDf = True, \n  \
    \                       probs = [0.1, 0.5, 0.75, 0.99])\nprint(predRes.iloc[0:6,\
    \ :].to_markdown())\n```\n\n    |    |       0.1 |       0.5 |   0.75 |   0.99\
    \ |\n    |---:|----------:|----------:|-------:|-------:|\n    |  0 | 0      \
    \   | 0.0107527 |   0.08 |   0.16 |\n    |  1 | 0         | 0.08      |   0.12\
    \ |   0.2  |\n    |  2 | 0.04      | 0.0967742 |   0.12 |   0.24 |\n    |  3 |\
    \ 0.056338  | 0.12      |   0.16 |   0.28 |\n    |  4 | 0.04      | 0.0967742\
    \ |   0.12 |   0.24 |\n    |  5 | 0.0666667 | 0.16      |   0.2  |   0.32 |\n\n\
    ## How to tune binSize parameter of LevelSetKDEx\n\n`dddex` also comes with the\
    \ class\n[`binSizeCV`](https://kaiguender.github.io/dddex/levelsetkdex.html#binsizecv)\n\
    that tunes the important *binSize* parameter via cross-validation in an\nefficient\
    \ manner. The class is designed in a very similar fashion to the\ncross-validation\
    \ classes of Scikit-Learn. As such, at first\n[`binSizeCV`](https://kaiguender.github.io/dddex/levelsetkdex.html#binsizecv)is\n\
    initialized with all the settings for the cross-validation.\n\n- **estimatorLSx**:\
    \ Either an object of class\n  [`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    \  or\n  [`LevelSetKDEx_kNN`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex_knn)\n\
    - **cvFolds**: An iterable yielding (train, test) splits as arrays of\n  indices\n\
    - **binSizeGrid**: The candidate values of *binSize* to evaluate\n- **probs**:\
    \ The probabilities for which quantiles are computed and\n  evaluated.\n- **refitPerProb**:\
    \ If True, for ever probability a fitted copy of\n  *estimatorLSx* with the best\
    \ binSize for the respective p-quantile is\n  stored in the attribute *bestEstimatorLSx*.\
    \ If False, only a single\n  fitted copy of *estimatorLSx* is stored with the\
    \ binSize that yielded\n  the lowest average aggregated costs over all quantile\
    \ estimations.\n- **n_jobs**: How many cross-validation split results to compute\
    \ in\n  parallel.\n\nAfter specifying the settings, `fit` has to be called to\
    \ compute the\nresults of the cross validation. The performance of every *binSize*\n\
    candidate value is being evaluated by computing the relative reduction\nof the\
    \ pinball loss in comparison to the quantile estimations generated\nby *SAA* (Sample\
    \ Average Approximation) for every quantile.\n\n``` python\nfrom dddex.levelSetKDEx\
    \ import binSizeCV\nfrom dddex.utils import groupedTimeSeriesSplit\n\ndataTrain\
    \ = dataYaz[dataYaz['label'] == 'train']\ncvFolds = groupedTimeSeriesSplit(data\
    \ = dataTrain, \n                                 kFolds = 3,\n              \
    \                   testLength = 28,\n                                 groupFeature\
    \ = 'id',\n                                 timeFeature = 'dayIndex')\n\nLSKDEx\
    \ = LevelSetKDEx(estimator = LGBM,\n                      binSize = None,\n  \
    \                    weightsByDistance = False)\n\nCV = binSizeCV(estimatorLSx\
    \ = LSKDEx,\n               cvFolds = cvFolds,\n               binSizeGrid = [20,\
    \ 100, 400, 1000],\n               probs = [0.01, 0.25, 0.5, 0.75, 0.99],\n  \
    \             refitPerProb = True,\n               n_jobs = 3)\n\nCV.fit(X = XTrain,\
    \ y = yTrain)\n```\n\nThe best value for *binSize* can either be computed for\
    \ every quantile\nseparately or for all quantiles at once by computing the average\
    \ cost\nreduction over all quantiles.\n\n``` python\nprint(f\"Best binSize over\
    \ all quantiles: {CV.bestBinSize}\")\nCV.bestBinSize_perProb\n```\n\n    Best\
    \ binSize over all quantiles: 1000\n\n    0.01    1000\n    0.25      20\n   \
    \ 0.50     100\n    0.75     100\n    0.99    1000\n    dtype: int64\n\nThe exact\
    \ results are also stored as attributes. The easiest way to view\nthe results\
    \ is given via `cv_results`, which depicts the average results\nover all cross-validation\
    \ folds:\n\n``` python\nprint(CV.cv_results.to_markdown())\n```\n\n    |     \
    \ |    0.01 |     0.25 |      0.5 |     0.75 |    0.99 |\n    |-----:|--------:|---------:|---------:|---------:|--------:|\n\
    \    |   20 | 3.23956 | 0.849528 | 0.808262 | 0.854069 | 2.46195 |\n    |  100\
    \ | 1.65191 | 0.857026 | 0.803632 | 0.835323 | 1.81003 |\n    |  400 | 1.64183\
    \ | 0.860281 | 0.812806 | 0.837641 | 1.57534 |\n    | 1000 | 1.54641 | 0.869606\
    \ | 0.854369 | 0.88065  | 1.52644 |\n\nThe attentive reader will certainly notice\
    \ that values greater than 1\nimply that the respective model performed worse\
    \ than SAA. This is, of\ncourse, simply due to the fact, that we didn\u2019t tune\
    \ the hyperparameters\nof the underlying `LGBMRegressor` point predictor and instead\
    \ used the\ndefault parameter values. The\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)classes\n\
    are able to produce highly accurate density estimations, but are\nobviously not\
    \ able to turn a terrible point predictor into a highly\nperformant conditional\
    \ density estimator. The performance of the\nunderlying point predictor and the\
    \ constructed\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    model go hand in hand.\n\nWe can also access the results for every fold separately\
    \ via\n`cv_results_raw`, which is a list with one entry per fold:\n\n``` python\n\
    CV.cv_results_raw\n```\n\n    [          0.01      0.25      0.50      0.75  \
    \    0.99\n     20    3.068598  0.854633  0.855041  0.953362  3.663885\n     100\
    \   1.626054  0.871327  0.833379  0.907911  2.591117\n     400   1.732673  0.860440\
    \  0.828015  0.890643  2.190292\n     1000  1.464534  0.873277  0.858563  0.891858\
    \  1.830334,\n               0.01      0.25      0.50      0.75      0.99\n  \
    \   20    4.157297  0.841141  0.795929  0.830544  1.883320\n     100   1.752709\
    \  0.862970  0.812126  0.819613  1.416013\n     400   2.085622  0.887758  0.839370\
    \  0.859290  1.296445\n     1000  1.767468  0.869484  0.860893  0.876293  1.464460,\n\
    \               0.01      0.25      0.50      0.75      0.99\n     20    2.492787\
    \  0.852811  0.773815  0.778301  1.838642\n     100   1.576956  0.836781  0.765390\
    \  0.778446  1.422947\n     400   1.107203  0.832645  0.771034  0.762992  1.239275\n\
    \     1000  1.407221  0.866058  0.843651  0.873799  1.284521]\n\nThe models with\
    \ the best *binSize* parameter are automatically computed\nwhile running `fit`\
    \ and can be accessed via `bestEstimatorLSx`. If\n`refitPerProb = True`, then\
    \ `bestEstimatorLSx` is a dictionary whose\nkeys are the probabilities specified\
    \ via the paramater *probs*.\n\n``` python\nLSKDEx_best99 = CV.bestEstimatorLSx[0.99]\n\
    predRes = LSKDEx_best99.predict(X = XTest,\n                                probs\
    \ = 0.99)\nprint(predRes.iloc[0:6, ].to_markdown())\n```\n\n    |    |   0.99\
    \ |\n    |---:|-------:|\n    |  0 |   0.32 |\n    |  1 |   0.32 |\n    |  2 |\
    \   0.32 |\n    |  3 |   0.32 |\n    |  4 |   0.32 |\n    |  5 |   0.32 |\n\n\
    ## Benchmarks: Random Forest wSAA\n\nThe `dddex` package also contains useful\
    \ non-parametric benchmark models\nto compare the performance of the\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    models to other state of the art non-parametric models capable of\ngenerating\
    \ conditional density estimations. In a [meta analysis\nconducted by S. Butler\
    \ et\nal.](https://ml-eval.github.io/assets/pdf/ICLR22_Workshop_ML_Eval_DDNV.pdf)\n\
    the most performant model has been found to be [weighted sample average\napproximation\n\
    (wSAA)](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253) based\non *Random\
    \ Forest*. This model has been implemented in a Scikit-Learn\nfashion as well.\n\
    \n``` python\nfrom dddex.wSAA import RandomForestWSAA\nRF = RandomForestWSAA()\n\
    ```\n\n[`RandomForestWSAA`](https://kaiguender.github.io/dddex/wsaa.html#randomforestwsaa)\n\
    is a class derived from the original `RandomForestRegressor` class from\nScikit-Learn,\
    \ that has been extended to be able to generate conditional\ndensity estimations\
    \ in the manner described by Bertsimas et al.\_in their\npaper [*From Predictive\
    \ to prescriptive\nanalytics*](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253).\n\
    The *Random Forest* modell is being fitted in exactly the same way as\nthe original\
    \ *RandomForestRegressor*:\n\n``` python\nRF.fit(X = XTrain, y = yTrain)\n```\n\
    \nIdentical to the\n[`LevelSetKDEx`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex)\n\
    and\n[`LevelSetKDEx_kNN`](https://kaiguender.github.io/dddex/levelsetkdex.html#levelsetkdex_knn)\n\
    classes, an identical method called `getWeights` and `predict`are\nimplemented\
    \ to compute conditional density estimations and quantiles.\nThe output is the\
    \ same as before.\n\n``` python\nconditionalDensities = RF.getWeights(X = XTest,\n\
    \                                     outputType = 'summarized')\n\nprint(f\"\
    probabilities: {conditionalDensities[0][0]}\")\nprint(f\"demand values: {conditionalDensities[0][1]}\"\
    )\n```\n\n    probabilities: [0.07592857 0.15247619 0.24953463 0.15449675 0.21877381\
    \ 0.09259957\n     0.03252381 0.01666667 0.007     ]\n    demand values: [0. \
    \  0.04 0.08 0.12 0.16 0.2  0.24 0.28 0.32]\n\n``` python\npredRes = RF.predict(X\
    \ = XTest,\n                     probs = [0.01, 0.5, 0.99],\n                \
    \     outputAsDf = True)\nprint(predRes.iloc[0:6, :].to_markdown())\n```\n\n \
    \   |    |   0.01 |   0.5 |   0.99 |\n    |---:|-------:|------:|-------:|\n \
    \   |  0 |      0 |  0.12 |   0.28 |\n    |  1 |      0 |  0.12 |   0.32 |\n \
    \   |  2 |      0 |  0.12 |   0.32 |\n    |  3 |      0 |  0.16 |   0.32 |\n \
    \   |  4 |      0 |  0.12 |   0.32 |\n    |  5 |      0 |  0.2  |   0.32 |\n\n\
    The original `predict` method of the `RandomForestRegressor` has been\nrenamed\
    \ to `pointPredict`:\n\n``` python\nRF.pointPredict(X = XTest)[0:6]\n```\n\n \
    \   array([0.1148, 0.12  , 0.1368, 0.1412, 0.1408, 0.1868])\n"
  dev_url: https://kaiguender.github.io
  doc_url: https://kaiguender.github.io
  home: https://kaiguender.github.io
  license: Apache Software
  license_family: APACHE
  summary: The package 'data-driven density estimation x' (dddex) turns any standard
    point forecasting model into an estimator of the underlying conditional density
build:
  noarch: python
  number: '0'
  script: '{{ PYTHON }} -m pip install . -vv'
extra:
  recipe-maintainers:
  - kaiguender
requirements:
  host:
  - pip
  - python
  - packaging
  - fastcore>=1.5.27
  - pandas>=1.3.0
  - sklearn
  - tsfresh>=0.19.0
  - lightgbm>=3.3.2
  - tabulate>=0.8.10
  run:
  - pip
  - python
  - packaging
  - fastcore>=1.5.27
  - pandas>=1.3.0
  - sklearn
  - tsfresh>=0.19.0
  - lightgbm>=3.3.2
  - tabulate>=0.8.10
test:
  imports:
  - dddex
