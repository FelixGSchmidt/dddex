{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61516f21-8606-49d3-8907-c0190315ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd6b76-2644-458b-8436-da808dd134c7",
   "metadata": {},
   "source": [
    "# Level-Set Based Kernel Density Estimation\n",
    "> Defining the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` which turn any point predictor into a conditional kernel density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7668fd2-5361-43a1-ba2b-42506a2419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp levelSetKDEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8cef21-4158-4307-b898-6bc03398a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d95d3a6-a5e0-46de-b191-d1de0281f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "\n",
    "import ipdb\n",
    "\n",
    "from dddex.basePredictor import BasePredictor, restructureWeightsDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8315b-c072-4e14-b477-5856b5d922fe",
   "metadata": {},
   "source": [
    "In the following we define the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` where KDE is short for 'Kernel Density Estimator' and the 'x' is supposed to signal that both classes can be defined based on any arbitrary point predictor. The name 'LevelSet' stems from the fact that every approach presented in this notebook interprets the values of the point forecasts as a similarity measure between samples. The point predictor is specified by the argument `estimator` and must have a `.predict()`-method and should have been trained before hand. \n",
    "\n",
    "Both classes `LevelSetKDEx` and `LevelSetKDEx_kNN` fulfill the same task: By first running `.fit(XTrain, yTrain)` and then calling `.generateWeights(XTest)`, they both output an estimation of the conditional density of every sample specified by 'XTest'. The basic idea for both approaches is also identical: Suppose we have a single test sample at hand. At first, we compare the value of the point prediction of this sample and the values of the point predictions of the training samples computed via `estimator.predict(XTrain)` and `estimator.predict(XTest)`, respectively. Based on this comparison, we select 'binSize'-many training samples that we deem the most similar to the test sample at hand. The concrete way we select the training samples constitutes the only difference between `LevelSetKDEx` and `LevelSetKDEx_kNN`. Finally, the empirical distribution of the y-values of these training samples then acts as our estimation of the conditional distribution.\n",
    "\n",
    "Further details on how both approaches work approaches can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a75d0-8a2c-4482-b372-6e67bf394d86",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Bin Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cebd8e-52d0-4a7f-af8f-75af78972824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx(BasePredictor):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # (Fitted) object with a .predict-method.\n",
    "                 binSize: int = None # Size of the bins created to group the training samples.\n",
    "                 ):\n",
    "        \n",
    "        if not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if not (isinstance(binSize, (int, np.integer)) or binSize is None):\n",
    "            raise ValueError(\"'binSize' has to be integer (or None if it is supposed to be tuned)!\")\n",
    "        else:\n",
    "            self.binSize = binSize\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.y = None\n",
    "        self.yPred = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"LevelSetKDEx(estimator = {self.estimator}, binSize = {self.binSize})\"\n",
    "    __repr__ = __str__      \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix used by 'estimator' to predict 'y'.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the features 'X'.\n",
    "            ):\n",
    "\n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        y = np.array(y)\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        \n",
    "        indicesPerBin, lowerBoundPerBin = generateBins(binSize = self.binSize,\n",
    "                                                       yPred = yPred)\n",
    "\n",
    "        self.y = y\n",
    "        self.yPred = yPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.lowerBoundPerBin = lowerBoundPerBin\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix of samples for which conditional density estimates are computed.\n",
    "                   outputType: 'all' | # Specifies structure of output.\n",
    "                               'onlyPositiveWeights' | \n",
    "                               'summarized' | \n",
    "                               'cumulativeDistribution' | \n",
    "                               'cumulativeDistributionSummarized' = 'onlyPositiveWeights', \n",
    "                   scalingList: list | np.ndarray | None = None, # List or array with same size as self.y containing floats being multiplied with self.y.\n",
    "                   ):\n",
    "        \n",
    "        binPerPred = np.searchsorted(a = self.lowerBoundPerBin, v = self.estimator.predict(X), side = 'right') - 1\n",
    "        neighborsList = [self.indicesPerBin[binIndex] for binIndex in binPerPred]\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8a8982-2492-463b-93c1-6a4eb0db2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fdb5bf-f39d-470f-b7cc-8679e733b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cca538d-96ff-48df-9e29-8573a6442a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661e147-5acc-4593-b3a5-e93f2d82447f",
   "metadata": {},
   "source": [
    "#### Generate Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeff05d2-7747-437c-9fcc-ebeaec04b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generateBins(binSize: int, # Size of the bins of values being grouped together.\n",
    "                 yPred: np.ndarray, # 1-dimensional array of predicted values.\n",
    "                 ):\n",
    "    \"Used to generate the bin-structure induced by the Level-Set-Forecaster algorithm\"\n",
    "    \n",
    "    yPredUnique = pd.Series(yPred).unique()\n",
    "    predIndicesSort = np.argsort(yPredUnique)\n",
    "    \n",
    "    yPredUniqueSorted = yPredUnique[predIndicesSort]\n",
    "    indicesByPred = [np.where(pred == yPred)[0] for pred in yPredUniqueSorted]\n",
    "    \n",
    "    currentBinSize = 0\n",
    "    binIndex = 0\n",
    "    trainIndicesLeft = len(yPred)\n",
    "    indicesPerBin = defaultdict(list)\n",
    "    lowerBoundPerBin = dict()\n",
    "    \n",
    "    for i in range(len(indicesByPred)):\n",
    "        \n",
    "        if i == 0:\n",
    "            lowerBoundPerBin[binIndex] = np.NINF\n",
    "            \n",
    "        currentBinSize += len(indicesByPred[i])\n",
    "        \n",
    "        indicesPerBin[binIndex].extend(indicesByPred[i])\n",
    "        \n",
    "        trainIndicesLeft -= len(indicesByPred[i])\n",
    "        \n",
    "        if trainIndicesLeft < binSize:          \n",
    "            for j in np.arange(i+1, len(indicesByPred), 1):\n",
    "                indicesPerBin[binIndex].extend(indicesByPred[j])\n",
    "            break\n",
    "\n",
    "        if currentBinSize >= binSize:\n",
    "            lowerBoundPerBin[binIndex + 1] = (yPredUniqueSorted[i] + yPredUniqueSorted[i+1]) / 2\n",
    "            binIndex += 1\n",
    "            currentBinSize = 0\n",
    "            \n",
    "    indicesPerBin = {binIndex: np.array(indices) for binIndex, indices in indicesPerBin.items()}\n",
    "    \n",
    "    lowerBoundPerBin = pd.Series(lowerBoundPerBin)\n",
    "    lowerBoundPerBin.index.name = 'binIndex'\n",
    "    \n",
    "    indices = np.array([])\n",
    "    for k in range(len(indicesPerBin.keys())):\n",
    "        indices = np.append(indices, indicesPerBin[k])\n",
    "\n",
    "    if len(indices) != len(yPred):\n",
    "        ipdb.set_trace()\n",
    "\n",
    "    \n",
    "    return indicesPerBin, lowerBoundPerBin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd550e-ae6e-44fe-a91b-f0ffee0096bc",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6155ad90-4e15-40ae-8e84-6b9394babd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_kNN(BasePredictor):\n",
    "    \"\"\"\n",
    "     `LevelSetKDEx_kNN` turns any point predictor that has a .predict-method \n",
    "    into an estimator of the condititional density of the underlying distribution.\n",
    "    The basic idea of each level-set based approach is to interprete the point forecast\n",
    "    generated by the underlying point predictor as a similarity measure of samples.\n",
    "    In the case of the `LevelSetKDEx_kNN` defined here, for every new samples\n",
    "    'binSize'-many training samples are computed whose point forecast is closest\n",
    "    to the point forecast of the new sample.\n",
    "    The resulting empirical distribution of these 'nearest' training samples are \n",
    "    viewed as our estimation of the conditional distribution of each the new sample \n",
    "    at hand.\n",
    "    \n",
    "    NOTE 1: The `LevelSetKDEx_kNN` class can only be applied to estimators that \n",
    "    have been fitted already.\n",
    "    \n",
    "    NOTE 2: In contrast to the standard `LevelSetKDEx`, it is possible to apply\n",
    "    `LevelSetKDEx_kNN` to arbitrary dimensional point predictors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Object with a .predict-method (fitted).\n",
    "                 binSize: int | None = None, # Size of the neighbors considered to compute conditional density.\n",
    "                 ):\n",
    "        \n",
    "        if not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if not isinstance(binSize, (int, np.integer)):\n",
    "            raise ValueError(\"'binSize' has to be integer!\")\n",
    "        else:\n",
    "            self.binSize = binSize\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.y = None\n",
    "        self.yPred = None\n",
    "        self.nearestNeighborsOnPreds = None\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"LevelSetKDEx_kNN(estimator = {self.estimator}, binSize = {self.binSize})\"\n",
    "    __repr__ = __str__   \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self:LevelSetKDEx_kNN, \n",
    "            X: np.ndarray, # Feature matrix used by 'estimator' to predict 'y'.\n",
    "            y: np.ndarray, # Target variable corresponding to features 'X'.\n",
    "            ):\n",
    "\n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "\n",
    "        yPred = self.estimator.predict(X)\n",
    "        yPred_reshaped = np.reshape(yPred, newshape = (len(yPred), 1))\n",
    "\n",
    "        nn = NearestNeighbors(algorithm = 'kd_tree')\n",
    "        nn.fit(X = yPred_reshaped)\n",
    "\n",
    "        #---\n",
    "\n",
    "        self.y = y\n",
    "        self.yPred = yPred\n",
    "        self.nearestNeighborsOnPreds = nn\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self: LevelSetKDEx_kNN, \n",
    "                   X: np.ndarray, # Feature matrix of samples for which conditional density estimates are computed.\n",
    "                   outputType: 'all' | # Specifies structure of output.\n",
    "                               'onlyPositiveWeights' | \n",
    "                               'summarized' | \n",
    "                               'cumulativeDistribution' | \n",
    "                               'cumulativeDistributionSummarized' = 'onlyPositiveWeights', \n",
    "                   scalingList: list | np.ndarray | None = None, # List or array with same size as self.y containing floats being multiplied with self.y.\n",
    "                   ):\n",
    "\n",
    "        nn = self.nearestNeighborsOnPreds\n",
    "\n",
    "        #---\n",
    "\n",
    "        yPred = self.estimator.predict(X)   \n",
    "        yPred_reshaped = np.reshape(yPred, newshape = (len(yPred), 1))\n",
    "\n",
    "        distancesDf, neighborsMatrix = nn.kneighbors(X = yPred_reshaped, \n",
    "                                                     n_neighbors = self.binSize + 1)\n",
    "\n",
    "        #---\n",
    "\n",
    "        neighborsList = list(neighborsMatrix[:, 0:self.binSize])\n",
    "        distanceCheck = np.where(distancesDf[:, self.binSize - 1] == distancesDf[:, self.binSize])\n",
    "        indicesToMod = distanceCheck[0]\n",
    "\n",
    "        for index in indicesToMod:\n",
    "            distanceExtremePoint = np.absolute(yPred[index] - self.yPred[neighborsMatrix[index, self.binSize-1]])\n",
    "\n",
    "            neighborsByRadius = nn.radius_neighbors(X = yPred_reshaped[index:index + 1], \n",
    "                                                    radius = distanceExtremePoint, return_distance = False)[0]\n",
    "            neighborsList[index] = neighborsByRadius\n",
    "\n",
    "        #---\n",
    "\n",
    "        # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08104da-f4fb-4b87-b86c-66867fb6cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9daa50-ef7b-4858-b343-b212f07a02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx_kNN.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97f4603-b305-464c-a67b-45ff7be439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(LevelSetKDEx_kNN.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dafad-822e-4399-9dac-c2c07e14a870",
   "metadata": {},
   "source": [
    "## LSF Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cb9efbd-4d21-4bc5-8bd9-82015cbb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class binSizeCV:\n",
    "\n",
    "    def __init__(self,\n",
    "                 estimator, # Object with a .predict-method (fitted).\n",
    "                 cv, # Specifies cross-validation-splits. Identical to 'cv' used for cross-validation in sklearn.\n",
    "                 LSF_type: 'LSF' | 'LSF_kNN', # Specifies which LSF-Object we work with during cross-validation.\n",
    "                 binSizeGrid: list | np.ndarray = [4, 7, 10, 15, 20, 30, 40, 50, 60, 70, 80, \n",
    "                                                   100, 125, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900,\n",
    "                                                   1000, 1250, 1500, 1750, 2000, 2500, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000], # binSize (int) values being evaluated.                \n",
    "                 probs: list | np.ndarray = [i / 100 for i in range(1, 100, 1)], # list or array of floats between 0 and 1. p-quantiles being predicted to evaluate performance of LSF.\n",
    "                 refitPerProb: bool = False, # If True, for each p-quantile a fitted LSF with best binSize to predict it is returned. Otherwise only one LSF is returned that is best over all probs.\n",
    "                 n_jobs: int | None = None, # number of folds being computed in parallel.\n",
    "                 ):\n",
    "        \n",
    "        # CHECKS\n",
    "        \n",
    "        if isinstance(estimator, (LevelSetKDEx, LevelSetKDEx_kNN)):\n",
    "            raise ValueError(\"'estimator' has to be a point predictor and not a LevelSetKDEx-Object!\")   \n",
    "        elif not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "            \n",
    "        if LSF_type is None or not LSF_type in [\"LSF\", \"LSF_kNN\"]:\n",
    "            raise ValueError(\"LSF_type must be specified and must either be 'LSF' or 'LSF_kNN'!\")\n",
    "        else:\n",
    "            self.LSF_type = LSF_type\n",
    "            \n",
    "        if np.any(np.array(probs) > 1) or np.any(np.array(probs) < 0): \n",
    "            raise ValueError(\"probs must only contain numbers between 0 and 1!\")\n",
    "        else:\n",
    "            self.probs = probs\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        self.binSizeGrid = binSizeGrid        \n",
    "        self.cv = cv\n",
    "        self.refitPerProb = refitPerProb\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.best_binSize = None\n",
    "        self.best_binSize_perProb = None\n",
    "        self.best_estimatorLSF = None\n",
    "        self.cv_results = None\n",
    "        self.cv_results_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f38ec08-8835-4c32-9979-5a71d90fd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(binSizeCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cee798f7-e38d-4fe1-a359-f16ae255f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: binSizeCV, \n",
    "        X, \n",
    "        y):\n",
    "    \n",
    "    scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(scoresForFold)(cvFold = cvFold,\n",
    "                                                                          binSizeGrid = self.binSizeGrid,\n",
    "                                                                          probs = self.probs,\n",
    "                                                                          estimator = self.estimator,\n",
    "                                                                          LSF_type = self.LSF_type,\n",
    "                                                                          y = y,\n",
    "                                                                          X = X) for cvFold in cvFolds)    \n",
    "\n",
    "    self.cv_results_raw = scoresPerFold\n",
    "\n",
    "    #---\n",
    "\n",
    "    nvCostsMatrix = scoresPerFold[0]\n",
    "\n",
    "    for i in range(1, len(scoresPerFold)):\n",
    "        nvCostsMatrix = nvCostsMatrix + scoresPerFold[i]\n",
    "\n",
    "    nvCostsMatrix = nvCostsMatrix / len(cvFolds)\n",
    "\n",
    "    self.cv_results = nvCostsMatrix\n",
    "\n",
    "    #---\n",
    "\n",
    "    meanCostsDf = nvCostsMatrix.mean(axis = 1)\n",
    "    binSizeBestOverall = meanCostsDf.index[np.argmax(meanCostsDf)]\n",
    "    self.best_binSize = binSizeBestOverall\n",
    "\n",
    "    binSizeBestPerProb = nvCostsMatrix.idxmax(axis = 0)\n",
    "    self.best_binSize_perProb = binSizeBestPerProb\n",
    "\n",
    "    #---\n",
    "\n",
    "    if self.refitPerProb:\n",
    "\n",
    "        LSFDict = dict()\n",
    "        for binSize in binSizeBestPerProb.unique():\n",
    "\n",
    "            if self.LSF_type == 'LSF':\n",
    "                LSF = LevelSetKDEx(estimator = self.estimator, \n",
    "                                         binSize = binSize)\n",
    "            else:\n",
    "                LSF = LevelSetKDEx_kNN(estimator = self.estimator, \n",
    "                                             binSize = binSize)\n",
    "\n",
    "            LSF.fit(X = X, y = y)\n",
    "            LSFDict[binSize] = LSF\n",
    "\n",
    "        self.best_estimatorLSF = {prob: LSFDict[binSizeBestPerProb.loc[prob]] \n",
    "                                  for prob in binSizeBestPerProb.index}\n",
    "\n",
    "    else:\n",
    "        if self.LSF_type == 'LSF':\n",
    "            LSF = LevelSetKDEx(estimator = self.estimator, \n",
    "                                     binSize = binSizeBestOverall)\n",
    "        else:\n",
    "            LSF = LevelSetKDEx_kNN(estimator = self.estimator, \n",
    "                                         binSize = binSizeBestOverall)\n",
    "\n",
    "        LSF.fit(X = X, y = y)\n",
    "\n",
    "        self.best_estimatorLSF = LSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6475323-2d18-489d-88f2-1675a3f7198f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx.py#L482){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### binSizeCV.fit\n",
       "\n",
       ">      binSizeCV.fit (X, y)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx.py#L482){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### binSizeCV.fit\n",
       "\n",
       ">      binSizeCV.fit (X, y)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(binSizeCV.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7bd61-adb2-42cc-93f9-9734d6d39f57",
   "metadata": {},
   "source": [
    "#### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf5e17b4-a9ca-4d19-a8db-c2b6879cbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def scoresForFold(cvFold, binSizeGrid, probs, estimator, LSF_type, y, X):\n",
    "   \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    estimator.fit(X = XTrainFold, y = yTrainFold)\n",
    "    \n",
    "    #---\n",
    "       \n",
    "    SAA_fold = SAA()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "    \n",
    "    # By setting 'X = None', the SAA results are only computed for a single observation (they are independent of X anyway).\n",
    "    # In order to receive the final dataframe of SAA results, we simply duplicate this single row as many times as needed.\n",
    "    quantilesDictSAAOneOb = SAA_fold.predict(X = None, probs = probs, outputAsDf = False)\n",
    "    quantilesDictSAA = {prob: np.repeat(quantile, len(XTestFold)) for prob, quantile in quantilesDictSAAOneOb.items()}\n",
    "    \n",
    "    #---\n",
    "                                                   \n",
    "    coefPresPerBinSize = list()\n",
    "    \n",
    "    binSizeGrid = [binSize for binSize in binSizeGrid if binSize <= len(YTrainFold)]\n",
    "    \n",
    "    for binSize in iter(binSizeGrid):\n",
    "        \n",
    "        if LSF_type == 'LSF':\n",
    "            estimatorLSF = LevelSetKDEx(estimator = estimator,\n",
    "                                              binSize = binSize)\n",
    "        else:\n",
    "            estimatorLSF = LevelSetKDEx_kNN(estimator = estimator,\n",
    "                                                  binSize = binSize)\n",
    "        \n",
    "        estimatorLSF.fit(X = XTrainFold,\n",
    "                         y = yTrainFold)\n",
    "        \n",
    "        quantilesDict = estimatorLSF.predictQ(X = XTestFold,\n",
    "                                              probs = probs,\n",
    "                                              outputAsDf = False)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # coeffPres = Coefficient of Prescriptiveness\n",
    "        \n",
    "        coefPresDict = {prob: [] for prob in probs}\n",
    "        \n",
    "        for prob in probs:            \n",
    "            coefPres = getCoefPres(decisions = quantilesDict[prob], \n",
    "                                   decisionsSAA = quantilesDictSAA[prob], \n",
    "                                   yTest = yTestFold, \n",
    "                                   prob = prob)\n",
    "            \n",
    "            coefPresDict[prob].append(coefPres)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    coefPresDf = pd.DataFrame(coefPresDict, index = binSizeGrid)\n",
    "    \n",
    "    return coefPresDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f5c4-fab2-4788-8691-ffbef17d703f",
   "metadata": {},
   "source": [
    "##### Coefficient of Prescriptiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a49f19-5f88-4600-bb1d-ff237b7daf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def getCoefPres(decisions, decisionsSAA, yTest, prob):\n",
    "\n",
    "    # Newsvendor Costs of our model\n",
    "    cost = np.array([prob * (yTest[i] - decisions[i]) if yTest[i] > decisions[i] \n",
    "                     else (1 - prob) * (decisions[i] - yTest[i]) \n",
    "                     for i in range(len(yTest))]).sum()\n",
    "    \n",
    "    # Newsvendor Costs of SAA\n",
    "    costSAA = np.array([prob * (yTest[i] - decisionsSAA[i]) if yTest[i] > decisionsSAA[i] \n",
    "                        else (1 - prob) * (decisionsSAA[i] - yTest[i]) \n",
    "                        for i in range(len(yTest))]).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # We have to capture the special case of costSAA == 0, because then we can't compute the \n",
    "    # Coefficient of Prescriptiveness using the actual definition.\n",
    "    if costSAA > 0:\n",
    "        coefPres = 1 - cost / costSAA\n",
    "    else:\n",
    "        if cost == 0:\n",
    "            coefPres = 1\n",
    "        else:\n",
    "            coefPres = 0\n",
    "    \n",
    "    return coefPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
