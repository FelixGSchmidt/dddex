{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class Weights-Based Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseWeightsBasedPredictor(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        \"\"\"Define weights-based predictor\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit weights-based predictor on given training data\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getWeightsData(self, X, scalingList = None):\n",
    "        \"\"\"Compute weights of feature array X\"\"\"\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self, X, probs = [0.1, 0.5, 0.9], outputAsDf = False, scalingList = None):\n",
    "        \n",
    "        distributionDataList = self.getWeightsData(X = X,\n",
    "                                                   outputType = 'cumulativeDistribution',\n",
    "                                                   scalingList = scalingList)\n",
    "        \n",
    "        quantilesDict = {prob: [] for prob in probs}\n",
    "        \n",
    "        for probsDistributionFunction, YDistributionFunction in distributionDataList:\n",
    "        \n",
    "            for prob in probs:\n",
    "                quantileIndex = np.where(probsDistributionFunction >= prob)[0][0]\n",
    "                quantile = YDistributionFunction[quantileIndex]\n",
    "                quantilesDict[prob].append(quantile)\n",
    "        \n",
    "        quantilesDf = pd.DataFrame(quantilesDict)\n",
    "        \n",
    "        # Just done to make the dictionary contain arrays rather than lists of the quantiles.\n",
    "        quantilesDict = {prob: np.array(quantiles) for prob, quantiles in quantilesDict.items()}\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if outputAsDf:\n",
    "            return quantilesDf\n",
    "        \n",
    "        else:\n",
    "            return quantilesDict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAA(BaseWeightsBasedPredictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Y = None\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeightsData(self, X = None, outputType = 'onlyPositiveWeights', scalingList = None):\n",
    "        \n",
    "        if X is None:\n",
    "            neighborsList = [np.arange(len(self.Y))]\n",
    "        else:\n",
    "            neighborsList = [np.arange(len(self.Y)) for i in range(X.shape[0])]\n",
    "        \n",
    "        # weightsDataList is a list whose elements correspond to one test prediction each. \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     Y = self.Y,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Weights Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def restructureWeightsDataList(weightsDataList, outputType = 'onlyPositiveWeights', Y = None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function. Creates weights-output by specifying considered\n",
    "    neighbors of training observations for every test observation of interest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neighborsList : {list}\n",
    "        The i-th list-entry is supposed to correspond to the i-th test observation. \n",
    "        Every list-entry should be a array containing the indices of training observations\n",
    "        which were selected as the neighbors of the considered test observation based on\n",
    "        the selected Level-Set-Forecaster algorithm.     \n",
    "    outputType : {\"summarized\", \"onlyPositiveWeights\", \"all\"}, default=\"onlyPositiveWeights\"\n",
    "        Specifies the structure of the output. \n",
    "        - If \"all\", then the weights are outputted as an array that is exactly as long as \n",
    "          the number of training observations. Consequently, also weights equal to zero are\n",
    "          being computed. \n",
    "          NOTE: This can be take up lots of RAM for large datasets with\n",
    "          > 10^6 observations.\n",
    "        - If \"onlyPositiveWeights\", then weights equal to zero are truncated. In order to be \n",
    "          able to identify to which training observation each weight belongs, a tuple is\n",
    "          outputted whose first entry are the weights and the second one are the corresponding\n",
    "          training indices. \n",
    "        - If \"summarized\", then additionally to \"onlyPositiveWeights\", weights referencing to the\n",
    "          same y-value are condensed to one single weight. In this case, the second entry of the\n",
    "          outputted tuple contains the y-values to which each weight corresponds. \n",
    "          NOTE: Summarizing the weights can be very computationally burdensome if roughly the considered\n",
    "          dataset has more than 10^6 observations and if ``binSize`` > 10^4.\n",
    "        - If \"cumulativeDistributionSummarized\", then additionally to \"summarized\", the cumulative sum of the\n",
    "          weights is computed, which can be interpreted as the empirical cumulative distribution\n",
    "          function given the feature vector at hand.\n",
    "          NOTE: This output type requires summarizing the weights, which can be very computationally \n",
    "          burdensome if roughly the considered dataset has more than 10^6 observations and if \n",
    "          ``binSize`` > 10^4.\n",
    "    Y: array, default=None\n",
    "        The target values of the training observations. Only needed when ``outputType`` is given as \n",
    "        \"all\" or \"summarized\".\"\"\"\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(Y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, YUnique = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                              YWeightPos = YWeightPos,\n",
    "                                                              equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YUnique = YUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, YUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistribution':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            indicesSort = np.argsort(YWeightPos)\n",
    "            \n",
    "            weightsPosSorted = weightsPos[indicesSort]\n",
    "            YWeightPosSorted = YWeightPos[indicesSort]\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsPosSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YWeightPosSorted = YWeightPosSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, YWeightPosSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistributionSummarized':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, YWeightPos = weightsDataList[i][0], Y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarizedSorted, YPosWeightUniqueSorted = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                                                   YWeightPos = YWeightPos,\n",
    "                                                                                   equalWeights = equalWeights)\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsSummarizedSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                YPosWeightUniqueSorted = YPosWeightUniqueSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, YPosWeightUniqueSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Weights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarizeWeightsData(weightsPos, YWeightPos, equalWeights = False):\n",
    "    \n",
    "    if equalWeights:\n",
    "        counterDict = Counter(YWeightPos)\n",
    "        YUniqueSorted = np.sort(list(counterDict.keys()))\n",
    "\n",
    "        weightsSummarizedSorted = np.array([counterDict[value] / len(YWeightPos) for value in YUniqueSorted])\n",
    "    \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "\n",
    "        for i, item in enumerate(YWeightPos):\n",
    "            duplicationDict[item].append(i)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        YUnique = list()\n",
    "\n",
    "        for value, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            YUnique.append(value)\n",
    "\n",
    "        weightsSummarized, YUnique = np.array(weightsSummarized), np.array(YUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        indicesSort = np.argsort(YUnique)\n",
    "        weightsSummarizedSorted, YUniqueSorted = weightsSummarized[indicesSort], YUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, YUniqueSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFinalOutput(dataOriginal, \n",
    "                        dataDecisions, \n",
    "                        demandFeature = 'actuals', \n",
    "                        mergeOn = None, \n",
    "                        variablesToAdd = None, \n",
    "                        rescaleDemandFeature = False, \n",
    "                        includeTraining = False, \n",
    "                        sortBy = None,\n",
    "                        longFormat = False,\n",
    "                        **kwargs):\n",
    "    \n",
    "    dataOriginal = dataOriginal.rename(columns = {demandFeature: 'actuals'})\n",
    "    \n",
    "    if rescaleDemandFeature:\n",
    "        if 'scalingValue' in dataOriginal.columns:\n",
    "            dataOriginal['actuals'] = dataOriginal['actuals'] * dataOriginal['scalingValue']\n",
    "        else:\n",
    "            raise ValueError(\"If 'rescaleDemandFeature' is set as 'True', a column\"\n",
    "                             \"named 'scalingValue' has to be part of 'data!'\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Adding additional data to the matrix dataDecisions\n",
    "    # NOTE: This function is written to be useable for all datasets we currently use. \n",
    "    colsToAdd = ['id', 'sku_code_prefix', 'sku_code', 'SKU_API',\n",
    "                 'actuals', 'revenue', 'label',\n",
    "                 'adi', 'adi_sku', 'adi_product', 'cv2', 'cvDemand_sku', 'cvDemand_product']\n",
    "    \n",
    "    if isinstance(mergeOn, list):\n",
    "        colsToAdd = colsToAdd + mergeOn\n",
    "    else:\n",
    "        colsToAdd.append(mergeOn)\n",
    "    \n",
    "    if isinstance(variablesToAdd, list):\n",
    "        colsToAdd = colsToAdd + variablesToAdd\n",
    "    else:\n",
    "        colsToAdd.append(variablesToAdd)\n",
    "        \n",
    "    if isinstance(sortBy, list):\n",
    "        colsToAdd = colsToAdd + sortBy\n",
    "    else:\n",
    "        colsToAdd.append(sortBy)\n",
    "    \n",
    "    colsToAdd = pd.Series(colsToAdd)\n",
    "    colsToAdd = colsToAdd[colsToAdd.isin(dataOriginal.columns)] \n",
    "    colsToAdd = colsToAdd.unique()\n",
    "    \n",
    "    dataTestInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'test', colsToAdd].reset_index(drop = True)\n",
    "    \n",
    "    if not longFormat:\n",
    "\n",
    "        if mergeOn is None:\n",
    "            dataResults = pd.concat([dataTestInfoToAdd, dataDecisions.reset_index(drop = True)], axis = 1)\n",
    "        else:\n",
    "            dataResults = pd.merge(dataTestInfoToAdd, dataDecisions, on = mergeOn)\n",
    "\n",
    "        #---\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dataDecisionsStacked = dataDecisions.stack().reset_index().set_index('level_0')\n",
    "        dataDecisionsStacked.rename(columns = {'level_1': 'decisionType'}, inplace = True)\n",
    "        \n",
    "        dataDecisionsStacked.rename(columns = {0: 'decisions'}, inplace = True)\n",
    "        dataDecisionsStacked.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        numberOfDecisionTypes = len(dataDecisionsStacked['decisionType'].unique())\n",
    "        \n",
    "        infoDuplicatedDf = dataTestInfoToAdd.loc[dataTestInfoToAdd.index.repeat(numberOfDecisionTypes)]\n",
    "        infoDuplicatedDf.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        dataResults = pd.concat([infoDuplicatedDf, dataDecisionsStacked], axis = 1)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    infoToAdd = pd.DataFrame(kwargs, index = [0])\n",
    "    infoToAdd['label'] = 'test'\n",
    "    dataResults = pd.merge(dataResults, infoToAdd, on = 'label')\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if includeTraining:\n",
    "        dataTrainInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'train', colsToAdd].reset_index(drop = True)\n",
    "        dataResults = pd.concat([dataTrainInfoToAdd, dataResults], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if not sortBy is None:\n",
    "        \n",
    "        if not all([sortByCol in dataResults.columns for sortByCol in sortBy]):\n",
    "            raise ValueError(\"Columns specified by 'sortBy' must be part of 'dataOriginal'!\")\n",
    "        else:\n",
    "            dataResults.sort_values(by = sortBy, axis = 0, inplace = True, ignore_index = True)\n",
    "    \n",
    "    return dataResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
