{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fcaa-9d2d-49ea-8cc3-b7e7ebd4b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d834faa-e24c-4542-9070-b8ec8562cc63",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101f7c6-4c66-481f-b580-a8dda687c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294fa26-4c16-435e-9d6a-b7537d8c4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24f328-6b14-4ae8-8034-c24c4a128b28",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e0e08-4fd9-4644-8048-d9f0948600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f316b2-dd73-4a57-aad7-ec6145bae9df",
   "metadata": {},
   "source": [
    "## Grouped Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5b30e-671f-4ce4-910c-f8eac47aee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function creates the cross-validation folds for every time series. Usually you'd want all test-observations \n",
    "# of each fold to refer to the same time period, but this is impossible to ensure in the case of the two-step models,\n",
    "# because the regression of the non-zero observations will always contain data of different time points. For that\n",
    "# reason, we refrain from trying to ensure this consistency.\n",
    "# Instead we organize our splits such that we always move a fixed amount of observations into the future from split\n",
    "# to split for every time series. This fixed amount of observations is currently set to the test length of the\n",
    "# corresponding time series.\n",
    "\n",
    "# In case this function is supposed to be used in the two-step case, data simply has to be filtered before hand\n",
    "# to only contain positive demand observations.\n",
    "\n",
    "def groupedTimeSeriesSplit(data, kFolds, testLength, groupFeature, timeFeature):\n",
    "    \n",
    "    # We reset the index because we have to access 'group.index' later on and\n",
    "    # want to make sure that we return the implicit numerical indices for our splits.\n",
    "    data = data.reset_index(drop = True)\n",
    "\n",
    "    dataGrouped = data.groupby(groupFeature)\n",
    "    splitNumbers = np.flip(np.array(range(kFolds)))\n",
    "    \n",
    "    foldsList = list()\n",
    "\n",
    "    for i in splitNumbers:\n",
    "\n",
    "        trainIndicesList = list()\n",
    "        valIndicesList = list()\n",
    "\n",
    "        valIndicesDict = dict()\n",
    "\n",
    "        for name, group in dataGrouped:\n",
    "\n",
    "            timeMin = int(group[timeFeature].min())\n",
    "            timeMax = int(group[timeFeature].max())\n",
    "\n",
    "            validationTimeMax = timeMax - i * testLength\n",
    "            trainTimeMax = validationTimeMax - testLength\n",
    "\n",
    "            trainTimesGroup = np.array(range(timeMin, trainTimeMax + 1))\n",
    "            valTimesGroup = np.array(range(trainTimeMax + 1, validationTimeMax + 1))\n",
    "\n",
    "            trainIndicesCheck = [timePoint in trainTimesGroup for timePoint in group[timeFeature]]\n",
    "            valIndicesCheck = [timePoint in valTimesGroup for timePoint in group[timeFeature]]\n",
    "\n",
    "            trainIndicesGroup = group.index[trainIndicesCheck]\n",
    "            valIndicesGroup = group.index[valIndicesCheck]\n",
    "\n",
    "            trainIndicesList.append(trainIndicesGroup)\n",
    "            valIndicesList.append(valIndicesGroup)\n",
    "\n",
    "        trainIndices = np.concatenate(trainIndicesList)\n",
    "        valIndices = np.concatenate(valIndicesList)\n",
    "        fold = (trainIndices, valIndices)\n",
    "\n",
    "        foldsList.append(fold)\n",
    "\n",
    "    return foldsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d688792-2db3-476c-a73a-f1088f4880ee",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940af8bb-aa80-4f46-adef-c3fd6a424a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generateFinalOutput(dataOriginal, \n",
    "                        dataDecisions, \n",
    "                        targetVariable = 'demand', \n",
    "                        mergeOn = None, \n",
    "                        variablesToAdd = None, \n",
    "                        scaleBy = None, \n",
    "                        includeTraining = False, \n",
    "                        sortBy = None,\n",
    "                        longFormat = False,\n",
    "                        **kwargs):\n",
    "    \n",
    "    dataOriginal = dataOriginal.rename(columns = {targetVariable: 'actuals'})\n",
    "    \n",
    "    if not scaleBy is None:\n",
    "        if not isinstance(scaleBy, str): \n",
    "            raise ValueError(\"'scaleBy' has to a string specifying a single feature to scale the target variable!\")\n",
    "        elif scaleBy in dataOriginal.columns:\n",
    "            dataOriginal['actuals'] = dataOriginal['actuals'] * dataOriginal[scaleBy]\n",
    "        else:\n",
    "            raise ValueError(f\"The specified feature {scaleBy} is not part of 'dataOriginal'!\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Adding additional data to the matrix dataDecisions\n",
    "    # NOTE: This function is written to be useable for all datasets we currently use. \n",
    "    colsToAdd = ['id', 'sku_code_prefix', 'sku_code', 'SKU_API',\n",
    "                 'actuals', 'revenue', 'label',\n",
    "                 'adi', 'adi_sku', 'adi_product', 'cv2', 'cvDemand_sku', 'cvDemand_product']\n",
    "    \n",
    "    if isinstance(mergeOn, list):\n",
    "        colsToAdd = colsToAdd + mergeOn\n",
    "    else:\n",
    "        colsToAdd.append(mergeOn)\n",
    "    \n",
    "    if isinstance(variablesToAdd, list):\n",
    "        colsToAdd = colsToAdd + variablesToAdd\n",
    "    else:\n",
    "        colsToAdd.append(variablesToAdd)\n",
    "        \n",
    "    if isinstance(sortBy, list):\n",
    "        colsToAdd = colsToAdd + sortBy\n",
    "    else:\n",
    "        colsToAdd.append(sortBy)\n",
    "    \n",
    "    colsToAdd = pd.Series(colsToAdd)\n",
    "    colsToAdd = colsToAdd[colsToAdd.isin(dataOriginal.columns)] \n",
    "    colsToAdd = colsToAdd.unique()\n",
    "    \n",
    "    dataTestInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'test', colsToAdd].reset_index(drop = True)\n",
    "    \n",
    "    if not longFormat:\n",
    "\n",
    "        if mergeOn is None:\n",
    "            dataResults = pd.concat([dataTestInfoToAdd, dataDecisions.reset_index(drop = True)], axis = 1)\n",
    "        else:\n",
    "            dataResults = pd.merge(dataTestInfoToAdd, dataDecisions, on = mergeOn)\n",
    "\n",
    "        #---\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dataDecisionsStacked = dataDecisions.stack().reset_index().set_index('level_0')\n",
    "        dataDecisionsStacked.rename(columns = {'level_1': 'decisionType'}, inplace = True)\n",
    "        \n",
    "        dataDecisionsStacked.rename(columns = {0: 'decisions'}, inplace = True)\n",
    "        dataDecisionsStacked.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        numberOfDecisionTypes = len(dataDecisionsStacked['decisionType'].unique())\n",
    "        \n",
    "        infoDuplicatedDf = dataTestInfoToAdd.loc[dataTestInfoToAdd.index.repeat(numberOfDecisionTypes)]\n",
    "        infoDuplicatedDf.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        dataResults = pd.concat([infoDuplicatedDf, dataDecisionsStacked], axis = 1)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    infoToAdd = pd.DataFrame(kwargs, index = [0])\n",
    "    infoToAdd['label'] = 'test'\n",
    "    dataResults = pd.merge(dataResults, infoToAdd, on = 'label')\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if includeTraining:\n",
    "        dataTrainInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'train', colsToAdd].reset_index(drop = True)\n",
    "        dataResults = pd.concat([dataTrainInfoToAdd, dataResults], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if not sortBy is None:\n",
    "        \n",
    "        if not all([sortByCol in dataResults.columns for sortByCol in sortBy]):\n",
    "            raise ValueError(\"Columns specified by 'sortBy' must be part of 'dataOriginal'!\")\n",
    "        else:\n",
    "            dataResults.sort_values(by = sortBy, axis = 0, inplace = True, ignore_index = True)\n",
    "    \n",
    "    return dataResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
